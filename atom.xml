<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[The Complaint of laoar]]></title>
  <link href="http://laoar.github.io/atom.xml" rel="self"/>
  <link href="http://laoar.github.io/"/>
  <updated>2014-06-10T23:09:01+08:00</updated>
  <id>http://laoar.github.io/</id>
  <author>
    <name><![CDATA[Yafang Shao]]></name>
    <email><![CDATA[laoar.shao@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hello Github!]]></title>
    <link href="http://laoar.github.io/blog/2014/05/25/hello-github/"/>
    <updated>2014-05-25T21:08:29+08:00</updated>
    <id>http://laoar.github.io/blog/2014/05/25/hello-github</id>
    <content type="html"><![CDATA[<p>Hi there all:</p>

<p>博客服务器由国外收费服务器迁移到github上，博客管理工具由wordpress转向octopress，这样显得Geek些。</p>

<p>Thanks<br>
<font color=blue>@laoar</font></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[性能优化：一些很有意思的尝试]]></title>
    <link href="http://laoar.github.io/blogs/501"/>
    <updated>2014-05-24T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/501</id>
    <content type="html"><![CDATA[<p>Cliff跟我说，做性能优化就得花时间去琢磨，<wbr />你要天天琢磨这个事，慢慢的就能找到办法了。 Boss的相对宽容，就给了我足够的时间来做一些琢磨（<wbr />一开始写的是研究，想着自己也不是啥科学家，<wbr />研究这个词还是太高大上了），<wbr />我也得以能够做一些实验来证明自己的想法。<br />
我在前一篇博客里也说过，<wbr />关于性能优化的经验文章可谓汗牛充栋数不胜数，<wbr />然而这一类的文章大多数都是纸上谈兵，纯理论派，<wbr />实际操作性太差。所以我就说下我的实际操作经验，<wbr />这些操作经验在理论派达人眼里可能是小儿科，<wbr />远不及他们指点江山挥斥方遒的派头，<wbr />然而实用的终究要比花拳绣腿好。<br />
因为有些东西我还没有想明白，<wbr />所以这篇博客可能最终没有真正的结论，只有实际的效果。<wbr />仅举几个有趣的例子。<br />
某松柏公司的版本管理方案大致是下面这个这个样子，<wbr />首先有一个主线版本，在每个迭代的开始会拉出一个开发分支，<wbr />开发分支开发完毕再sync到主线，<wbr />然后再从主线拉出一个发布分支做版本发布。注：<wbr />开发分支主要是针对一些大的特性的改动，一些bug fix可能只是commit到主线，<wbr />而不会commit到开发分支。<br />
对于开发分支，会定期有一些sanity check，即做一些测试来监控开发版本的一些指标，<wbr />比如性能指标。主线由于相对稳定，因而就没有这些sanity check。<br />
开发分支由于有daily build的性能测试，因而能够及时的发现问题，<wbr />所以它的性能状况一向都比较良好。<br />
于是问题就来了。最近再把开发分支sync到主线，<wbr />然后从主线拉出来一个发布分支后，发布分支的性能非常之低。 比如开发分支的UDP吞吐量是900Kpps，<wbr />发布分支的UDP吞吐量竟然下降到了780Kpps，<wbr />版本经理的预期是要达到850Kpps，<wbr />因而我需要找出来为什么性能下降这么大，<wbr />以及怎么样来提升它的性能。<br />
理论派达人如果过来指点下江山的话，可能会说，<wbr />用profile工具来比较性能较好（开发分支）和性能很差（<wbr />发布分支）的两个image，看看他们的关键路径上有什么差异，<wbr />先找出差异来，再做调整。 但是，实际情况是，性能差异这么巨大，<wbr />很大的可能性不是由于critical path的代码改动导致的，<wbr />而是由一些无关痛痒的代码改动导致cache的sensitiv<wbr />e。 如果花时间的话，这种想法也的确可行，<wbr />然而最终效果总是看起来很美实际上只是老大爷打太极有板有眼就是<wbr />敌不过一双拳。<br />
再稍微务实一点的想法是，<wbr />我们找到主线分支上性能较好的一个revision，<wbr />然后利用二分查找法来找出是不是由于某个check－<wbr />in导致的。这里的难点在于，首先，<wbr />代码的性能变化是一个波浪型的曲线，<wbr />你没有办法来确认你找到的性能较高的revision是一个合理<wbr />的revision。这种想法更务实一点，<wbr />花费的时间可能也会很多。<br />
更务实一点的想法是，我们找出哪些代码只commit到了主线（<wbr />性能较低）而没有commit到开发分支（性能很好），<wbr />那么性能下降必然是由这些check－in导致的。难点在于，<wbr />每天的代码的check－in少则有4、5个，多则有十几个，<wbr />这个比较也是一个吃力不讨好的事。<br />
所以，我们玩点有技术含量的吧，比较二进制！ 坦白说，在我打算这么做的时候，我不知道结果会怎么样，<wbr />也不知道能不能搞定，反正当时心一横，管它哪，干吧！<br />
于是，<wbr />我将开发分支和发布分支的两个image的符号表给dump出来<wbr />，然后由低地址到高地址sort排序。 大致命令如下：<br />
objdump -t image1.elf | sort &gt; iamge2.sym<br />
objdump -t image2.elf | sort &gt; image2.sym<br />
vim -d iamge1.sym image2.sym<br />
然后我痛苦的发现，这两个符号表竟然有10多万个差异。 这要是一个个的去查，那得查到天荒地老了。<br />
然而这些差异很多都是符号地址的差异，符号的大小基本都一样的。 所以接下来，我就把image1.sym和image2.<wbr />sym的地址这一列给删除再做比较。<wbr />在vim的visual模式下可以按列删除，<wbr />具体请google，此处不缀述。<br />
然后问题就稍微简化了些，很快就找到了一个值得怀疑的地方，<wbr />如下图。（为了不泄漏某松柏公司的信息，我对图片做了些处理。<wbr />虽然这些信息可能也无关紧要，不过做人要职业些嘛）<br />
<img src="http://laoar.github.io/images/501.jpg"></p>
<p>我们可以看到深蓝色的那一行就是两个的差异，<wbr />该符号的大小由0x910增大到了0xaa8，<wbr />共增加了102条指令。于是我验证了下，<wbr />果然是这个差异导致的性能由900Kpps下降到了780Kpp<wbr />s。<br />
既然知道了是哪里导致性能下降的了，问题就简单了，<wbr />肯定是有办法来解决的。<br />
不过我要说的，到这里并没有结束，接下来才是关键。事实上，<wbr />这个符号在data path里并不会执行到，也就是说，<wbr />UDP的吞吐量根本就不会执行这个函数，<wbr />但是这个函数的改动又切切实实的导致了性能的下降。<wbr />唯二的可能性就是a)这个符号的改动影响了后续符号的地址，<wbr />导致一些cache line对齐问题，b)这个符号的改动使得critical path里面的函数／数据出现了cache line的冲突。然而由于符号的差异确实太多了，<wbr />而且critical path里面的函数和数据也很多，<wbr />所以我没有太多的时间和精力来查找到底是哪个原因。<wbr />对于b这种可能性相对好验证一些，只要知道了cache line的大小，和组相连的set，就能够知道cache line冲突的单位是set*line_size, 然后以这个值为基本单位来比较相差这个大小的符号即可。<wbr />对于情况a，可能就不太好验证了，因为要排查的东西太多了，<wbr />时间不允许，不过对于这种情况，我们可以有一些预防措施，<wbr />比如使用__attribute__(signed(128))<wbr />来将一些大的数据结构cache line对齐。注意我们将大数据结构cache line对齐并不是说对齐可以提高访问速度，oh god，如果你这么想那就真的是太悲哀了，<wbr />RISC的访问必须是对齐访问，<wbr />CISC的访问可以不对齐访问多花一个cycle。<wbr />这里所说的cache line对齐，是指，避免这个大数据结构占用多个cache line，比如如果这个大数据结构是136字节，<wbr />不对齐的话它完全是有可能占用3个cache line的（假设cache line的大小是128字节），而对齐的话只需要2个cache line。接下来如果我有充足的时间的话，<wbr />我会继续深入的做一下这件事的琢磨，看看能不能发现一些东西。<wbr />当然也可能花费很多时间什么都做不出来。<br />
事情到这里还没有结束。我们还得再琢磨琢磨。<br />
我们也看到了，<wbr />这明显是无关代码改动导致关键路径性能下降的例子，<wbr />那么我们为何不将关键路径，<wbr />或者说主要功能的代码给放到符号地址空间的前面，<wbr />而将这些次要功能或者说不是很重要的代码给放到后面，<wbr />这样这些无关代码的改动就不会太明显的影响了关键路径了。<wbr />这也说明，对于大型软件系统而言，<wbr />合理的划分地址空间是多么的重要，<wbr />不然以后就只能疲于奔命的去解决这些很无奈的问题了。<br />
合理的划分地址空间，就设计到Makefile／<wbr />链接脚本的设计。这些也不是一个想当然的事，<wbr />而是要根据具体情况来做。 在设计大型软件之初，也不必要去过多的关注这方面，<wbr />等到需要改变的时候再去改变也不迟，毕竟，高司令也教导我们，“<wbr />过早优化是万恶之源”。</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[test]]></title>
    <link href="http://laoar.github.io/blog/2014/05/20/test/"/>
    <updated>2014-05-20T22:38:39+08:00</updated>
    <id>http://laoar.github.io/blog/2014/05/20/test</id>
    <content type="html"><![CDATA[<p>test</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[从linux内核里来学习性能优化，和一个例子]]></title>
    <link href="http://laoar.github.io/blogs/482"/>
    <updated>2014-04-29T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/482</id>
    <content type="html"><![CDATA[<p style="padding-left: 30px;"></p>
<p>    我对“案例”这个词比较反感，所以标题是“一个例子”。 “案例”这个词，就跟“拉通”、“沟通”、“跟产品PK”、“奋斗者”、“客户”一样，在我眼里属于洗脑文化，所以我比较忌讳使用这些词。使用“例子”、“员工”、“customer”这些词(很抱歉我不知道“拉通”、“沟通”、“和产品PK”是什么意思，因而找不到对应的词，还好我生活和工作中也不会用到这些词)会让我更舒服一些。<br />
<p style="padding-left: 30px;"></p><br />
    对于性能优化，相信稍微懂点编程的人都能说出个二五六来。比如对于inline的使用，只要是会写C语言，相信你一定能够口若悬河滔滔不绝的说出一大坨一大坨inline的优劣以及对性能的影响，然而这一大坨一大坨的经验都是bullshit，一切要看结果怎么样（我心里忽然不由自主的冒出了“以结果为导向”这个词，oh god，愿上帝拯救我）。就像木心在《文学回忆录》说的，“思想过剩的人，行动力往往较差”，关于性能优化这件事也是这样，不要迷信前人的经验，要自己动手去做。注意“迷信”的前提是了解，你要去了解他们的说法，但是不要去相信他们。<span style="line-height: 1.5em;">   </span></p>
<p><span style="line-height: 1.5em;">    再来看inline这个问题，有个哥哥根据自己的实验发现，</span><span style="text-decoration: underline;"><a href="https://lwn.net/Articles/82494/">在skbbuff里面的一些skb__函数都不应该使用inline，因为去掉这些inline后能够提升3%的性</a>能</span><span style="line-height: 1.5em;">。所以说，这些想当然的经验都是bullshit，一切要看实际结果，写代码时不要傻逼兮兮的一上来就到处加inline，毕竟高司令也说过，“过早优化是万恶之源‘。</span><br />
<p style="padding-left: 30px;"></p><br />
    当然我要说的例子不是这个inline的例子。<br />
<p style="padding-left: 30px;"></p><br />
    我们知道Linux Kernel跟GNU是密切联系的，所以在Linux Kernel里面到处可见GCC的一些优化手段，比如GCC的attribute这个东西。《GNU/Linux Application Programming》的作者Tim Jones在他的文章《GCC hacks in Linux Kernel》里对GCC的这些手段做了些总结，写的也挺好。不过这个老兄忽略了一个很重要的东西，__attribute__((section(&#8220;name&#8221;))). 如果你要是写过内核驱动或者做过内核启动的话，你应该对__attribute__((section(&#8220;.init.text&#8221;)))不会陌生，没错他就是__init这个宏。__init这个宏的作用是，Gcc会把这个函数放在.init.text的输入段给链接器，这样所有以__init来声明的符号都会放在.init.text这个section里面。然后在初始化完毕，这些初始化代码显然就不会再执行了，那么他们占用的内存就可以被释放掉，所以在kernel初始化结束会调用一个free_initmem()函数来释放所有位于.init这个section的函数。<br />
<p style="padding-left: 30px;"></p><br />
    不过可惜的是，由于释放的是代码段的页表，因而必须得在内核里面来做，而且内核也没有提供这样的系统调用给用户态 ，对于用户态而言就没有办法来这样处理。事实上有很多用户态程序的初始化代码也很大，几百KB的初始化代码也是很正常的，释放这部分空间也是很可观。不清楚内核开发者为什么不考虑将这个方法以一个系统调用的形式导出到用户态。即提供这样一个系统调用：<br />
int free_inittext(unsigned long start, unsigned long end);<br />
释放页表要求是页对齐，这部分工作可以在内核里面进行检查，并将start向后对齐，以及end向前对齐，该系统调用的返回值是实际释放的page数目，如果没有释放就返回负值。<br />
<p style="padding-left: 30px;"></p><br />
    我们已经知道__attribute__((section(“name&quot;)))的作用是将这个函数给放在一起，这样就给我们提供了一个优化思路，我们完全可以将hot function用这种方法给放在一起，来减少icache miss。之所以是将hot function放在一起，而不是将逻辑上顺序执行的代码顺序排放，是因为icache的替换算法是LRU，即最近最少使用。既然是hot function，显然是会经常调用的，那么，我们把经常调用的函数给放在一起，当某一个函数得到执行时就会可能将另外的hot function一并给预取到cache line里面，其实本质上就是利用cpu的指令预取特性，有点类似于likely()；并且由于这个cache line里都是hot instruction，它总是会得到执行，被替换出去的可能性就大大减少，从而提高cache hit rate。这里需要澄清的一点是，当执行到某一个函数的时候，由于cpu不直接跟memory打交道，它会把该函数读取到cache里面再load到寄存器里面去执行，它把函数读取到cache里面时并不是把整个函数给读取到cache里，而是只读取一个cache line。比如我某一个函数它的起始地址是0x40000008, 假设cache line大小是32bytes， 那么我要执行这个函数的时候就会一次性的将0x40000000～0x40000020这部分的指令给读取到cache，某些cpu会有critical设计，即先读取0x40000008开始的4字节（对于32bits的CPU而言，其read path是4字节）读取到cache接下来再读取其余的28bytes。<br />
<p style="padding-left: 30px;"></p><br />
    这里就是我要说的一个例子。我在做性能优化的时候，仅仅是将critical path里的2个函数利用这种方法给放在了一起，就将UDP的throughput给提升了&gt;10%.<br />
<p style="padding-left: 30px;"></p><br />
    然而，我之所以选择这样做也是迫不得已，我更理想的想法是将另外一个函数给定义成inline，然而无奈另外一个函数有多处调用，如定义成inline可能会得不偿失，因为可执行文件的size就会变大了。结果也恰如我所料，将其定义为inline反而导致性能下降。<br />
<p style="padding-left: 30px;"></p><br />
    所以我就想，C语言里面是否应该有这种设计：inline不是用来作为函数定义的限制词，而是作为函数调用的限制词，即我在调用的时候来决定是否将该函数给内嵌过来，而不是在定义该函数的时候限制其为inline。 比如：<br />
<div class='bogus-wrapper'><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="kt">int</span> <span class="nf">func_c</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">b</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="kt">void</span> <span class="nf">func_a</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>    <span class="c1">//...</span>
</span><span class='line'>    <span class="n">c</span> <span class="o">=</span> <span class="n">func_b</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="kr">__inline</span><span class="p">;</span>
</span><span class='line'>    <span class="c1">//...</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></div></p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于struct hack, 优雅的FreeBSD]]></title>
    <link href="http://laoar.github.io/blogs/466"/>
    <updated>2014-03-20T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/466</id>
    <content type="html"><![CDATA[<p style="padding-left: 30px;"></p>
<p>    本来今晚计划去健身的，谁知道正打算去的时候发现背包里只带着运动裤，忘了带跑步鞋了，脚上的Northface鞋实在不适合健身，于是无奈放弃了。然后趁这个机会写点东西。<br />
<p style="padding-left: 30px;"></p><br />
    好了，言归正传。<br />
<p style="padding-left: 60px;"></p><br />
    我们对于struct stack熟知的是下面这种情况。<br />
<img src="http://laoar.github.io/images/466.jpg"><br />
我们定义一个结构体family来记录一个正常家庭的人的年龄，显然对于一个正常家庭来说，爸妈各只有一个，但是孩子有多少就不确定了，所以我们把family的最后一个成员baby定义为了变长数组。对于这种在一个结构体里面最后一个变量是变长数组而且仅有这一个成员是变长数组的情况，我们称之为struct hack。这属于C语言里面的一个trick。<br />
<p style="padding-left: 30px;"></p><br />
    我这边博客要讨论的不是这种情况，我要说的是我在FreeBSD内核里面看到的一个很有意思的代码，也是一种struct的trick。它起的名字是struct __hack, 所以我就联想到了前面的struc hack ：）<br />
<p style="padding-left: 30px;"></p><br />
    在说FreeBSD内核的struct __hack之前，先来说下Linux内核里的do{ }while(0) 。 在我之前一个<a href="http://www.laoar.net/?p=289">blog</a>里，我说了 do{} while(0) 的目的是为了防止悬挂else问题。之所以会出现悬挂else问题，就是因为代码写的不是太规范。所以说，对于if/else,我们还是要尽量写成<br />
<p style="text-align: left; padding-left: 150px;"><br />
if  (…)  {<br />
&#8230;<br />
} else {<br />
&#8230;<br />
}</p><br />
即使里面只有一句话。<br />
<p style="padding-left: 30px;">然后在来看这个struct __hack。</p><br />
<img src="http://laoar.github.io/images/466-2.jpg"><br />
<p style="padding-left: 90px;"></p><br />
<p style="text-align: left;">     任何技术或者技巧的出现都是为了解决一些问题的，那么这里的struct __hack是为了解决什么问题哪？ 其实它是用来告诉程序员这个宏只用作函数声明。我们知道函数声明一般都是下面这个样子：</p><br />
<p style="text-align: left; padding-left: 150px;"><br />
void foobar(void);</p><br />
<p style="text-align: left;">这个宏也是为了模拟这种形式, 当然事实上它仍然是宏定义了一个函数, 这里要模拟，是说，它的函数体在别处。比如：</p><br />
<p style="text-align: left; padding-left: 150px;">TASKQUEUE_DEFINE_THREAD(kqueue);</p><br />
<p style="padding-left: 30px;"></p><br />
    总之，这个小小的细节正体现出了FreeBSD所追求的优雅性.<br />
<p style="padding-left: 30px;"></p><br />
    写完发现字数太少了。所以在歪歪唧唧一些。最近写blog经常中英混杂，貌似好像大概可能也许很多人都讨厌这个样子，不过目测讨厌这样的多半都不是计算机这个行业或者工作中和英语打交道较少的。假如英语真的影响到你的薪水，进而又影响到你的生活质量，显然你也会不自觉的变成这样，无他，赚钱糊口而已。正应了那句话，叫做，屌丝的生活高富帅永远都不懂，只有经历了你才会明白：）<br />
<p style="padding-left: 30px;"></p><br />
    计算机这个东西本身就是美国的，凡事都是原汁原味的好，而且在绝大多数情况下，你面对的只有英文。比如对于一个C编译器的编译错误信息“discards qualifiers from pointer target type”， 如果你要Google一下才知道这是什么错误的话，那么恭喜你，码农的苦逼生活不适合你：）<br />
<p style="padding-left: 30px;"></p><br />
    其实反过来想也是这样。我在的小区附近老外特别多，旁边一个小区住着N多的韩国人。最近楼下新开了一家店，为了招徕老外，显然得起个英文名，它的英文名字叫“Chinese Humberger”，你猜它的中文名叫什么？ 肉夹馍！ 哈哈，很不伦不类，莫名其妙不是。所以说，原汁原味的才是最好的。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于Profiling]]></title>
    <link href="http://laoar.github.io/blogs/455"/>
    <updated>2014-03-06T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/455</id>
    <content type="html"><![CDATA[<p style="padding-left: 30px;"></p>
<p>    对于大型软件系统而言，可维护性是很重要的一个方面，否则到了最后没人能够搞懂全部的代码那就是一个灾难了。这方面的例子可谓比比皆是，比如华为VRP平台的Linux内核，因为模块间耦合太严重很难升级Linux内核版本，搞到最后实在搞不下去，于是团队解散，投奔中软的RTOS。松本行弘在《代码的未来》这本书里说，开发人员的成本越来越高，机器成本则是越来越便宜，所以我们的编程语言应该要侧重于开发效率和维护效率，而不应该太过于关注性能，至于性能这件事，完全可以通过升级更牛逼的硬件来很轻松的搞定。其实Ruby之父的这个观点在很久之前就被Donald Kunth说过了，“premature  optimization is the root of all evil”。两位大师的观点可谓殊途同归。<br />
<p style="padding-left: 30px;"></p><br />
    Donald Knuth在大多数软件工程师眼里可谓神一般的存在，它的这句名言也一直被码农们牢记在心。 Donald Knuth前辈并不是反对optimization，它只是反对premature optimization，即盲目无意义的优化，optimization应该要针对critical的东西。如何寻找critical的代码，就是一个profiling系统需要做的事情。当然profiling本身也属于可维护性的一部分。<br />
<p style="padding-left: 30px;"></p><br />
    Profiling的本质，是为了充分发挥CPU的能力。现代CPU的体系结构已经很牛逼了，但是相对而言，软件开发语言则没多大进步，再加上软件开发人员水平的良莠不齐，直接的后果就是，CPU的能力没有得到充分利用。虽然说使用更牛逼的CPU更牛逼的内存更牛逼的散热系统可以大幅的提高性能，但是在相同的配置下，比别人有更牛逼的表现不是更好嘛。iPhone和三星Andriod手机的对比不就是很好的证明嘛。<br />
<p style="padding-left: 30px;">现代CPU的流水线体系结构大概是这个样的。</p><br />
<img src="http://laoar.github.io/images/455.jpg"><br />
Instruction Fetch Unit用来从memory中取指令到寄存器中，Decode and Issue Unit则是将指令翻译成CPU能够识别的编码然后发射到Functional Units（比如运算器等等）来执行这些指令，FU的执行结果再放回到寄存器中。 所以一条指令的执行完毕最好的情况下需要4个CPU cycles就可以搞定了。我们都已经知道流水线的原理就是，指令A在FU执行时，指令B在译码准备发射，指令C在被从内存中读取到寄存器中，于是这样就实现了不同指令的并行工作来提高CPU的效率。 CPU的这个设计思想被Unix开发人员学来了，于是就是有了管道（pipe）。再接着Erlang又把管道的原理用在了语言特性上来实现并发编程，最后Golang又把Erlang的管道特性给学了去。这并不是说CPU的设计人员有多么牛逼，而是说，软件开发人员要在CPU设计人员制定的规则下做事才能充分利用CPU的性能。<br />
<p style="padding-left: 30px;"></p><br />
    CPU的这种流水线存在的缺点，或者说，要依赖编程人员水平的地方在于，指令A可能是JUMP D，这样就导致指令B和指令C要被从流水线里面刷出去了，就造成CPU cycle的浪费。 要想避免这种浪费，就要要求程序自身具有很好的分支预测特性，即要充分迎合CPU的这种预测执行的特性。 在Linux内核里面随处可见的likely()/unlikely()就是做这个事情的，likely()/unlikely()是告诉编译器要去做什么以及不去做什么。因为CPU最后执行的是编译器生成的二进制，而不是我们写的代码，所以一个好的编译器对于程序性能的影响是巨大的。现在的编译器要比程序员聪明的多，它往往能够预测出程序接下来要执行的指令是什么，有时候我们加的likely()/unlikely()可能根本就没有意义甚至适得其反，因为程序员相比编译器而言太蠢了。有一个例子就是real-time Linux Kernel的maintainer Steven Rostedt做的一个统计，他发现内核里面有<a href="https://lwn.net/Articles/419102/">大量误用likely()/unlikely()的地方</a>，其中在<a href="https://lwn.net/Articles/420028/">page_mapping()函数里面的一个unlikely()有39%的概率是错误的</a>，然后这位大神就提交了一个patch把这个unlikely()给去掉了。所以，如果你没有十足的把握，就不要随便使用unlikely()/likely()这俩宏。 不过有一个基本准确的经验就是，在错误处理的分支上加unlikely()大致不会是坏事。比如：<br />
char * p = (char *)malloc(<span class="caps">SIZE</span>);<br />
if (unlikely(!p)) {<br />
perror(&#8220;malloc&#8221;);  // 原谅我没有缩进<br />
}</p>
<p>这里的unlikely()在绝大多数情况下都能够提升性能，不过，如果这段代码是non-critical的，那么这个unlikely()就是non-sense的，因为在这种情况下它对性能的提升可能近似为0.0000%，而且进一步而言，牛逼点的编译器自己也能够做这个事。如果这段代码真的是critical的话，那么这个unlikely()就有必要了。我们可以统计下linux内核的代码，unlikely()的使用次数大概是likely()的10倍，就是很多错误处理的分支都加了unlikely()的缘故。<br />
<p style="padding-left: 30px;"></p><br />
    除了这个分支预测之外，还有一个性质对流水线的影响较大，那就是数据依赖。指令B已经译码完成，准备要发出去的时候，发现它的操作数不可用，有可能别的CPU在使用这个操作数，于是指令B只能在这里傻傻的等待另外的CPU释放这个操作数。<br />
<p style="padding-left: 30px;"></p><br />
    所以对于流水线型的CPU，主要就这两方面决定着性能的好坏：1）数据依赖 2）分支预测。现在的并发编程也主要是解决的这两件事。<br />
<p style="padding-left: 30px;"></p><br />
    CPU的设计人员为了让程序员能够更直观的感受这两件事的表现，就给程序员提供了一些性能统计的寄存器，这些寄存器都在协处理0里面。他们做的事情，就是统计多长时间cpu cycle内，指令的issue/retire的数目，icache/dcache的hit/miss,以及l2 icache/dcache的hit/miss等等。比如说，如果icache的miss较大，那显然是程序的分支预测较差。<br />
<p style="padding-left: 30px;"></p><br />
    对于应用程序开发人员而言，是没有必要了解这么详细的性能寄存器信息的，或者说在这个上面花费精力意义不大。因而就有了perf、oprofile、systemtap、dtrace这些性能调试工具的应运而生。这些工具提供给程序员更友好的方式来分析程序，借助这些工具能够很直观的找到程序的critical部分。</p>
<p>注：<br />
<sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup> 图是用Google Doc画的，第一次使用Google Doc画图，画的较差:(<br />
<sup class="footnote" id="fnr2"><a href="#fn2">2</a></sup> 本文的观点纯属个人歪歪，未必准确，但基本正确（所以说，汉语很博大精深不是） ：）</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为什么发送segment fault信号的进程总是PID0 ？]]></title>
    <link href="http://laoar.github.io/blogs/435"/>
    <updated>2014-02-15T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/435</id>
    <content type="html"><![CDATA[<p>让我们先来看一个小程序。<br />
<img src="http://laoar.github.io/images/435.jpg"><br />
这个小程序的大概意思就是，注册了一个segmentation fault的handler， 在该handler里把SIGSEGV信号的一些信息打印出来，我这里打印了两个值，一个是进程自身的PID，另外一个是info-&gt;si_pid, Linux内核里对它的解释是发送信号的进程的PID。</p>
<p>来看下它的执行结果：<br />
<img src="http://laoar.github.io/images/435-2.jpg"><br />
我们可以看到当前进程的PID是1060，info-&gt;si_pid的值则是0。</p>
<p>我们看下&lt;sys/signal.h&gt;对si_pid的解释，<br />
<img src="http://laoar.github.io/images/435-3.jpg"><br />
这里清清楚楚的写的是，sending process，即发送该信号的进程ID。</p>
<p>于是，问题就来了，为什么SIGSEGV信号的发送进程是PID0 哪？</p>
<p>我们从头来理一下Kernel对signal的处理机制， 以我们前面这个小程序为例。</p>
<p>在我们的程序里，Line 17定义了一个指针p并初始化为NULL，接着在Line 22对p指向的内容赋值为1，我们都已经知道对空指针解引用会导致segment fault。那么，segment fault具体是怎么产生的？</p>
<p>Kernel为了提高App的执行效率采取的是惰性分配机制，即只有在第一次写的时候才会给它分配具体的物理内存。那么在Line 22由于还没有为指针p分配物理内存，所以这里会首先产生一个缺页异常(page fault), 通过缺页异常陷入内核，接着执行内核的缺页异常处理流程。在内核缺页异常处理流程里，它判断出这是一个用户态的缺页异常，于是就尝试该缺页异常是否可以通过一些手段来解决掉。很遗憾的是，由于没给p申请内存空间，所以p不属于合法的vma区域，即所谓的bad area。于是就产生一个SIGSEGV信号给我们的这个进程。于是异常流程就执行完毕了，开始返回用户态。在返回用户态之前，进程会判断是否有信号需要处理。由于在之前产生了一个SIGSEGV信号，所以又去执行SIGSEGV的信号处理流程了，注意此时已然是在user land来执行了。SIGSEGV的默认处理流程是产生一个segment fault，并且生成一个core文件。由于我们这个程序自己注册了一个SIGSEGV的信号处理程序void handler (int sig, siginfo_t *info, void *ctx), 从内核态返回后就开始执行这个handler函数。 在信号处理程序结束后，如果没有让程序退出，即没有那个exit(-1), 那么就返回到我们程序产生异常的地方，即Line 22，继续执行（PS : 这就是为什么如果没有这个exit，该程序就会死循环的原因）。 这就是Line 22这个语言在内核里的一些动作。</p>
<p>从我们的这个分析可以看出，应该不关PID0鸟事才会。可是事实现实，PID0确实插了一脚，很困惑不是，它到底插在哪儿了哪？</p>
<p>唯一的可能之处就是SIGSEGV产生的地方。我们打开Kernel代码来看看这里到底发生了什么。<br />
<img src="http://laoar.github.io/images/435-4.jpg"><br />
这一小段代码就是SIGSEGV产生的地方。可以看到这里对info的si_signo/si_error/si_addr进行了赋值，在前面还对si_code进行了赋值，即SIGSEGV信号只用到了这四个字段。 也就是说，对于SIGSEGV而言，si_pid啥意义都没有。 </span></p>
<p>我们可以对比看下SIGKILL这个信号。<br />
<img src="http://laoar.github.io/images/435-5.jpg"><br />
可以看到在产生SIGKILL的地方，将当前的活动进程赋值给了si_pid, 这也是我们在终端里按下CRTL+C来杀死一个进程的原理。接受键盘输入的进程(bash进程)给目标进程发送了一个SIGKILL信号，然后就把目标进程干掉了。</p>
<p>想了并研究了这么一段时间，却得出了这么一个结论（对SIGSEGV而言，si_pid没有任何意义也就没处理），表示很失望，Kernel工程师能够再爱岗敬业一些么？ 至少把那句注释“sending process”改称“sending process(if needed)” 也好么，至少不会引起太多的误解。</p>
<p>&nbsp;</p>
<p>PS. : 写完一看字数，又是1000字，看来很适合写千字文。不深不浅聊技术，千字文是也。</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[我的2013]]></title>
    <link href="http://laoar.github.io/blogs/422"/>
    <updated>2014-01-01T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/422</id>
    <content type="html"><![CDATA[<p><span style="line-height: 1.5;">    这一年，翻了很多书，走了很多路，看过不少电影，听过不少音乐。回过头来总结一下自己的2013，只谈风月，不论国是。 </span></p>
<h3> 一 书 （以阅读时间先后排序）</h3>
<table>
<tbody>
<tr>
            <th><strong>Name</strong></td>
            <th><strong>Author</strong></td>
            <th><strong>Rate</strong></td>
</tr>
<tr>
            <td>看见</td>
            <td>柴静</td>
            <td>3星</td>
</tr>
<tr>
			<td>全世界人民都知道</td>
			<td>李承鹏</td>
			<td>3星</td>
</tr>
<tr>
			<td>只是孩子</td>
			<td>帕蒂・史密斯</td>
			<td>3星</td>
</tr>
<tr>
			<td>梁启超传</td>
			<td>解玺璋</td>
			<td>2星</td>
</tr>
<tr>
			<td>夹边沟记事</td>
			<td>杨显惠</td>
			<td>5星</td>
</tr>
<tr>
			<td>中国近代史（下）港版</td>
			<td>徐中约 </td>
			<td>5星</td>
</tr>
<tr>
			<td>C陷阱与缺陷 </td>
			<td>凯尼格</td>
			<td>5星</td>
</tr>
<tr>
			<td>我的人生九局</td>
			<td>聂卫平</td>
			<td>3星</td>
</tr>
<tr>
			<td>定西孤儿院记事</td>
			<td>杨显惠</td>
			<td>5星</td>
</tr>
<tr>
			<td>Linux/Unix设计思想</td>
			<td>甘卡兹</td>
			<td>3星</td>
</tr>
<tr>
			<td>淘宝技术这十年</td>
			<td>子柳</td>
			<td>3星</td>
</tr>
<tr>
			<td>Linux内核设计的艺术</td>
			<td>新设计团队</td>
			<td>1星</td>
</tr>
<tr>
			<td>叫魂</td>
			<td>孔飞力</td>
			<td>3星</td>
</tr>
<tr>
			<td>The Design and Implementation of <br />
			the FreeBSD Operating System</td>
			<td>Marshall Kirk Mckusicd</td>
			<td>5星</td>
</tr>
<tr>
			<td>无非求碗热汤喝</td>
			<td>张佳玮</td>
			<td>4星</td>
</tr>
<tr>
			<td>七周七语言</td>
			<td>Bruce A.Tate</td>
			<td>4星</td>
</tr>
</tbody>
</table>
<p><span style="line-height: 1.5;">     总计16本书，平均3.25周看一本书。 其中有10本是非技术类书籍，6本是技术类书籍。13年看的书相比12年增加了3本，一本技术书，二本非技术书。 </span><br />
<span style="line-height: 1.5;">    每当看到别人总结的”XXXX年我的年度十大好书“之类的东西时，就感觉一阵汗颜，我也想弄这么一个列表，可是回过头一看自己一年也就那10来本书。</span><br />
<span style="line-height: 1.5;">    在2013年12月初的时候，我买了一个Kindle Paperwhite II。买这个kindle有一个主要原因就是发觉我的阅读量太少了，一年一年的没啥变化，所以索性买个kindle，这样，在地铁上、等公交时、排队中我都可以随手从口袋里掏出来看书了。而且，再也不用担心在拥挤的地铁里别人拿文艺青年的眼神来看你了，被爱书之人称作文艺青年是一种荣耀，被从不看书之人称作文艺青年则是一句笑话。kindle的尺寸恰好能够放在外衣（比如冲锋衣）的口袋里，很是便携，于是之前习惯从口袋里掏出iPhone刷微博也变成了拿出kindle来看书，套用马化腾的一句话就是，取代微博的一定不是另一个微博，是kindle。 </span><br />
<span style="line-height: 1.5;">    kindle也有很多缺点，比如书太少、技术类书籍跟纸质书差不多贵，不过这丝毫掩盖不了它的优势：1）随手看书 2）书即买即得。</span></p>
<h3>二 影</h3>
<table>
	<tr>
		<td>电影 </td>
		<td>23</td>
		<td> 其中在影院看了9部，占比40%  </td>
	</tr>
	<tr>
		<td>电视剧</td>
		<td> 15 </td>
		<td> 其中英剧8部，美剧7部 </td>
	</tr>
	<tr>
		<td>纪录片</td>
		<td> 1      </td>
		<td>  BBC纪录片 </td>
	</tr>
	<tr>
		<td>合计</td>
		<td>49</td>
		<td> </td>
</table>
<span>    今年看的电影数目相比去年大大下降，去年仅在影院就看了20部。13年第三季度由于跳槽至外企工作，所以看了一些美剧和英剧来提高自己的英语。事实上，看美剧还是有助于提高英语的，至少，假如你不幸也在朝阳门被老外闯红灯给撞了，当他用一口流利的北京话对你说“滚你丫妈X的”时， 你也能操着一口伦敦腔优雅的对他说“You, son of a bitch”. </span><p><span>    来到北京后，观影的体验确实发生了用翻天覆地形容也不为过的变化。在深圳看电影一般都是用各个影院的电影兑换券，这些兑换券随处有卖的。假如你跟你女朋友逛街累了想去看场电影，最简便省事又省钱的办法就是，到这个影院最近的报刊亭，然后说“哎，师傅，来两张东海太平洋的兑换券”就可以了。 在深圳，一般是一张兑换券换一张普通电影票，二张换一张3D的，三张换一张iMax的。深圳最好的影院毫无疑问是KKMall，时尚现代又有深圳最大的iMax。但是，如果是情侣看电影的话， 首选毋庸置疑是中影国际，地处欢乐海岸，毗邻红树林，逛、玩、游、憩、吃皆不勿，KKMall除了逛就是逛还是逛。</span></p>
<p><span>    在北京，FESCO每个季度会为外企员工发6张电影优惠券，不过1张只面值10元。各个影院对FESCO券的要求还不一样，比如，中影电影院4张FESCO券可换一张3D电影票，国安剧院是4张换一张2D的，星美则是6张换一张2D的。 然后就是，我发现北京的影院比较破旧，毫无深圳（关内）影院的时尚大气感。一个文艺美女告诉我说百老汇在北京是较好的影院，但是我感觉它在深圳属于不想去第二次的影院，因为破和旧。地处全宇宙中心的五道口电影院，跟90年代的录像厅一般无二，屏幕巨小，影厅还巨狭长。</span></p>
<h3>三 音</h3>
<p><span>    很意外的是，当我查看我的iPod的播放记录时，我发现终于有首歌曲被我单曲循环了超过100遍，它也是我iPod里唯一一首单曲循环过百的歌曲：Depapepe的《风向仪》。我在想，单曲循环超过1000次的歌曲会在什么时候出现？十年以后？</span></p>
<h3>四 路</h3>
<p><span>    这一年确实走了很多路，现在我依然走路上下班，假如我一天走5公里，那么一年就是1825公里。深圳到北京的距离是2400公里，也就是，我用了差不多一年半的时间从深圳走到了北京。 </span></p>
<p>&nbsp;</p>
<p><span>    2013年过去了，我很怀念它，希望我的每一年都如2013这般精彩。</span></p>
<p>&nbsp;</p>
<p>注：<br />
<sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup>  一开始是写在豆瓣上的<br />
<sup class="footnote" id="fnr2"><a href="#fn2">2</a></sup> <a href="http://www.laoar.net/blogs/227">我的2012</a></p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MacOS:mdworker曾让我很不爽]]></title>
    <link href="http://laoar.github.io/blogs/402"/>
    <updated>2013-12-15T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/402</id>
    <content type="html"><![CDATA[<p><span style="line-height: 1.5;">    一开始，我以为mdworker是“memory decompress worker”， 好吧，谁让我是linux内核出身的哪，正应了那句话，叫做“往往以自己的立场来揣测别人”。 事实上，它是“metadata server worker”。不过它让我很不爽的原因与它的名字无关，在我的Macbook刚升级到Mavericks的时候，大概是升级后还有很多善后工作要处理，它老是偷偷摸摸的运行，导致我看个优酷都卡的不行，你能体会到正到精彩处电脑却卡的连鼠标都动不了的那种心情。 需要声明的是，那段时间我在看《绝命毒师》，别想多。 </span></p>
<p><span style="line-height: 1.5;">     然后我就有了要干掉这个mdworker的念头。作为一个内核出身的工程师，自然是不屑于打开control panal，这里勾一下那里选一下的这种毫无技术含量的做法的。I do it on my own。我打算自己写个C代码来控制它，虽然这不是个好方法，甚至都不是个正确的方法，但是至少我在思考并且知道是可行的,而且我还能搞定它。</span></p>
<p><span style="line-height: 1.5;">    再然后我就发现我不太适合做一个freelance，我的这个想法一直拖拖拉拉都没动笔，过了段时间Maverick竟然很流畅了，mdworker也轻易不出山了，即使出山也没那么大动静了，这更让我没动力来写了。 </span></p>
<p><span style="line-height: 1.5;">    最后，有一天我感冒了，周末躺在床上什么都不想动，于是打开了电脑就开始写这个很无聊的工具了。 </span></p>
<p><span style="line-height: 1.5;">    我的目的是这样的：我要让这个进程的cpu占用率低于某个值，比如10%。</span></p>
<p><span style="line-height: 1.5;">    接着我就想该怎么来实现，虽然当时我躺在床上浑身无力，可是大脑还能正常运转，下面是我的思路：</span></p>
<p><span style="line-height: 1.5;">1） 根据mdworker这个名字来找到对应的进程<br />
</span>      关键点：<br />
<span style="line-height: 1.5;">      可能会有多个进程，注意是起多个进程，不是指多线程。</span></p>
<p><span style="line-height: 1.5;"><br />
</span>2） 我需要遍历整个进程链表来进行名字匹配，如果某个进程的名字mdworker，就认为找到了对应的进程。<br />
<span style="line-height: 1.5;">    关键点：<br />
</span>    bsd系统维护着两个进程链表，一个是allproc，一个是zomproc。zomproc是bsd的僵死进程链表，非僵死进程都在allproc链表里面。所谓僵死进程是指，这个进程挂掉了，但是它没有通知它的父进程，打个比方就是，你死了但是还没有去民政局里登记你的死亡，此时你就是处在僵死状态。<br />
要明白allproc和zomproc这两个链表的特性，我们只需要知道他们使用的是什么结构体就好了。他们两个使用的都是bsd系统的通用结构体LIST_ENTRY，于是我们就可以在心里默念一下这个结构体的特点，它是一个双向非循环链表，会使用一个全局变量header来标识这个链表，这个header会指向链表的头部和尾部，在通常情况下我们都是把新元素插入到链表的头部。我们知道系统中进程的PID是按照由小到大递增的，也就是现有PID0，然后是PID1&#8230; 于是我们就知道这个allproc和zomproc链表里的元素（进程）从尾部到头部的PID是逐渐增大的，当然如果fork的进程数超过了PID MAX，那就另说了。</p>
<p>3） 抓取到这个进程后，我们就可以使用它的pid来控制它了，如何来控制？<br />
<span style="line-height: 1.5;">    关键点：<br />
</span>    进程A如何来控制一个毫不相干的进程B？ 也许你一下子想不起来该怎么来实现，但是你再仔细想一想，这不就是GDB干的事嘛！gdb使用的ptrace系统调用来将目标进程变成自己的子进程，然后attach到这个子进程后就可以任意的玩这个子进程了，想必对于我们国人而言这个不太难理解，上司自然是可以控制自己的下属的，或者说，我要想让你听我的话得先把你变成我的下属。人之本性在技术层面也是被体现的淋漓尽致啊！     如果使用ptrace的话，在这里可能就有些大材小用了，我一开始设想是使用ptrace系统调用，后来发现实现起来相对复杂些。 然后我又想到，CTRL＋C不也是可以停止一个进程嘛，它使用的是信号，我可以使用signal来实现我的目的。 即，如果进程老是占用CPU，我就给它发一个stop信号来停止它，过段时间我再给它发个continue信号让它运行。当然前提是它可以接受信号。<br />
至于说让它停止多长时间和运行多长时间则是一个小学数学问题了。 比如我想让这个进程的CPU占用率不超过10%，那么只要检测到该进程的CPU占用率超过了10%，我就强制让它睡眠。假如我以1S为一个周期，我只要让进程的工作时间不超过100ms就可以了。</p>
<p>4） 如何计算一个进程的CPU占用率<br />
<span style="line-height: 1.5;">    关键点：<br />
</span><span style="line-height: 1.5;">    我的计算方式是，在时间点a我记录下进程执行了多长时间pa，然后在时间点b我再记录进程执行了多长时间pb。那么(pb-pa)/(b-a)就是这段时间进程的CPU占用率。 </span></p>
<p>5） 我要想控制别人，得比别人拥有更大的权利<br />
<span style="line-height: 1.5;">    关键点：<br />
</span><span style="line-height: 1.5;">    这个进程既然是控制进程，那么它得比被控制的进程具有更高的优先级来执行，不然被控制进程都执行完了那这个控制进程也没啥用了。<br />
</span>    首先，这个控制进程选用什么样的调度策略。Oh, come on! 当然是选用Normal了，如果选用FIFO或者RR不是会更影响整个系统的性能嘛！<br />
接着，normal进程可以通过调整它的nice值来调整它的优先级，nice的范围是（－20，＋20），－20具有最高优先级，＋20的优先级最低。<br />
最后，我们可以通过setpriority()这个函数来减小进程的nice值，不过，Only the super-user may lower priorities.（man手册）。也就是说，你要使用root权限。</p>
<p><span style="line-height: 1.5;">    有了上面的思路之后，我就开始动笔写了，虽然躺在床上浑身乏力，可是手指还算灵活，敲起键盘来也还能噼里啪啦。 </span></p>
<p><span style="line-height: 1.5;">    然后，我就在github上发现了一个类似的东西，叫做</span><a title="cpulimit" href="https://github.com/laoar/cpulimit"><span style="text-decoration: underline;">cpulimit</span></a><span style="line-height: 1.5;"> 。我看了一下它的代码，发现了很多bug，也想到了解决方案，后来看到这个项目只活跃了半年时间，而且都一年多没有commit了，就没有心思去给这个项目贡献点自己的力量了。 我的代码可以说是这个cpulimit的郭敬明版，虽然短小功能少，可是很强悍，正所谓浓缩的都是精华。 我正在着手将我的代码保存到github上时，忽然卡特打了个电话喊我一起打篮球。大概是写了点代码精神爽吧，虽然感冒没好完，我还是毫不犹豫的披挂上阵了。于是那个周日的下午，我和卡特,姚明一起在中科院电子所和其他几个人3v3了两三个小时，等到周一我起来去上班时感觉整个身子骨都散架了。 </span></p>
<p><span style="line-height: 1.5;">    在我打算写这篇博客时，也将这个郭敬明版的cpulimit给放到了github上：</span><a title="my-task-manager" href="https://github.com/laoar/my-task-manager"><span style="text-decoration: underline;">my－task－manager</span><span style="line-height: 1.5;"> 。 </span></a></p>
<p><span style="line-height: 1.5;">    github确实是个好东西，你想写点什么时，总能在上面找到个类似的做参考。 作为一个程序员，如果都不知道github的话，那就太过于不思进取了。</span></p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[我为什么从华为离职]]></title>
    <link href="http://laoar.github.io/blogs/393"/>
    <updated>2013-11-23T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/393</id>
    <content type="html"><![CDATA[<p><span style="line-height: 13px;">    最近知乎上有很多“我为什么从XX离职”这一类的问题，很火热，所以也忍不住想写点东西。 </span></p>
<p><span style="line-height: 1.5;">    有些事情现在不是太想说，也许等到成了个老头后，那个时候大家对这些事情都不太care了，再来写一写回忆录挺好的。  </span></p>
<p><span style="line-height: 1.5;">    有一次我去武汉出差，早上8点钟左右(大概是这个时间点，反正很早)的样子我到了武研所，然后去打卡。在我进门的时候，我忽然感觉有个人从里面出来与我擦肩而过很熟悉，由于我走的太快，所以一时没想起来是谁。 于是我在回忆了几秒钟后赶紧再走到大楼的外面，然后我就认出了那个熟悉的背影，他是我的第二个徒弟。他是去食堂吃早饭，看着他在寒风中渐渐远去的背影，我很想跑过去跟他打声招呼，“hey，现在怎么样，挺好的吧”。但是理智告诉我，我还是别去了，于是我就看着他的背影消失在我的视线里，心里很难受。 我之所以不敢上前跟他打招呼，是因为他的第一年考评成绩为C，C在华为意味着什么想必大家都清楚。在我知道他的成绩是C的时候，我很震惊，我不知道为什么会是C。那个时候他还没有转正，只来华为有4，5个月的样子。后来到了他的转正答辩，由于他转正答辩前转到了另外一个组，所以我跟另外一个组我们一起来决定最终给他的转正成绩是什么。另外一个组的人坚持要给他B＋，我就找各种理由给他B，因为我知道假如我同意给了他B＋，我以后在华为的路可能会很坎坷。最终他的成绩还是B，这是我违背良心做的一件事，从此以后我一直对这件事很惭愧，一直觉得愧对于他。在这里我并不是想说什么好什么不好，只是我的一个忏悔，或者说，愿上帝原谅我，我想与人为善。</span></p>
<p><span style="line-height: 1.5;">     在华为的经历，我的理解是成长的代价。这个时候是我由年轻热血到成熟稳重必须要经历的一个阶段，只不过这个阶段恰巧在华为而已（这句话是不是很熟悉，没错，孙董事长也说过），也许可能华为是加速了我的成长。如果再回到3年前，我会拒绝华为的offer，这样至少华为在我心里仍然是中国民族企业的骄傲。 </span></p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊Kernel：linux kernel和bsd kernel实现doubly-linked list的差异]]></title>
    <link href="http://laoar.github.io/blogs/367"/>
    <updated>2013-11-23T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/367</id>
    <content type="html"><![CDATA[<p><span style="line-height: 1.5;">    如果你google一下“linux list”，你会得到一堆各种各样对linux doubly-linked list的分析，以及各种各样对于这些分析的拷贝。所以当我打算写这个话题时，看到这个结果就放弃了，不想继续趟这躺浑水了，至少不想跟这些习惯于拷贝粘贴的家伙为伍。但是某一天，我实现的queue出现了一个问题，于是我就开始认认真真的研究起来双向链表在linux和bsd系统上实现的差异。然后我就打算尝试从一个不同的角度来探讨这个话题。</span></p>
<p><span style="line-height: 1.5;">    First thing first，实现的目的是为了应用，所以我打算从应用的角度慢慢写。很多分析linux list的文章呜哩哇啦一顿乱写，看的我们也是一头雾水莫名奇妙,这是大多数技术博客的惯病，我尽量避免步他们的后尘。为了写这篇blog，我先写了两个用例，一个是双向链表在linux系统上的应用，一个是在bsd系统上的应用，代码放在了我的github上：</span><a style="line-height: 1.5;" title="list paradigm" href="https://github.com/laoar/list" target="_blank"><span>list paradigm</span> </a><span style="line-height: 1.5;">（P.S. : paradigm这个单词是公司里的英语老师Michael教的，所以在这里忍不住想用一下）。再插播一句，有一次在英语课上Michael问我，“what&#8217;s PS”? 我支支吾吾了半天说“paste”， 顿时惹来Michael的一顿shame on me，“oh my god, it’s post script!” 于是我也学会了shame on you， 用老祖宗的话说就是“鄙视你”。</span></p>
<p><span style="line-height: 1.5;">    先简单解释下我写的这两个C文件。</span></p>
<p><strong>1. linux_list.c</strong></p>
<p><span style="line-height: 1.5;">    在linux系统上，它的/usr/include目录就是我们编译C代码的头文件所在路径的其中一个。这个目录里有一个特殊的子目录，叫做/usr/inlude/linux,  之所以叫linux这个名字，是因为它是从linux kernel里面导出来的头文件，而不是从glibc库里面导出来的头文件。按理说，linux kernel导出来的头文件是给kernel module用的，所以一般都放在kernel module的编译路径里，现在既然在/usr/include/目录里也放了一份，那目的很明显了，用户态程序想用就用吧。但是坑爹的是，linux kernel头文件不是你想用就能用，因为linux kernel是GPL协议，而Glibc是LGPL协议，也就是说如果你include了linux kernel头文件，那么你的代码也必须是GPL的，你只有copyleft，没有copyright，你的代码就别指望着卖钱了，乖乖开源吧。 而如果你是include的glibc的头文件，那么没关系，你无需开源，你具有你写的代码的copyright。 需要声明的是，我不是一个开源的反对者，相反我认为开源挺有好处的，把代码提交给社区，总有热心人会帮你挑错或者帮你改进代码，这不挺好嘛。但是，我也不是一个GPL的提倡者，GPL这个东西，多少感觉有点社会主义大锅饭或者共产主义的味道，大锅饭的后果是某些人钻空子不劳而获，而另外一些人会丧失劳动的积极性进而生产力得不到很好的发展。 Anyway，个人见解，不喜就喷。</span></p>
<p><span style="line-height: 1.5;">    To be continue，继续按理说，既然放在了/usr/include/ 这个目录下，那就是告诉应用程序开发者们想用就用吧，大不了开源。然而事情没有想象中的那么简单，就比如这个蛋疼的/usr/include/linux/list.h这个头文件，它竟然很操蛋的说“＃ifdef __KERNEL__”, 就是说，你是内核模块才能接着用后面这段代码。于是我被逼无奈在自己的C代码里面定义了这个宏，当然这是非常规手法，莫学，我只是为了写它的使用示例才这么干的。</span></p>
<p><span style="line-height: 1.5;">    至于gcc的－D选项，想必大家都明白，它是为了方便选择性编译用的。</span></p>
<p><strong>2. bsd_list.c</strong></p>
<p><span style="line-height: 1.5;">    好了，终于轮到新欢bsd了，linux已成了旧爱，所以我才敢在这里喷GPL，正所谓“mountain is high， empire is far”。</span></p>
<p><span style="line-height: 1.5;">    BSD的设计就很优雅，包括它的应用程序编译环境，不像linux那样一锅乱炖。 <span class="caps">BSD</span> kernel导出来的头文件是放在了 /usr/include/sys这个目录下，与linux list.h相对应的文件是queue.h,这个文件则是想用就能用，而且BSD协议也不要求你必须开源，所以尽管放心大胆的使用“＃include &lt;sys/queue.h&gt;”就好了。 另外，牛逼哄哄的Mac也是使用了很多BSD的组件，我的这个C文件就是在Mac上编译并执行的。当然，对于Mac而言，它有自己的sysroot路径，所以事实上queue.h是在/$%^&amp;<strong>!@###/sys/queue.h这个地方，$%^&amp;</strong>!@###这个鬼东西就是Mac的编译环境路径的名字，不必care。</span></p>
<p><span style="line-height: 1.5;">    说了那么多，还没有说到正题，好了，让我们言归正传。</span></p>
<p><span style="line-height: 1.5;">    我们知道，在网络系统上，不过就是收包和发包，所以Network里面对链表的使用主要就是添加和删除，很少涉及查找某一个元素或者把一个元素插入到中间某一个位置。对于添加和删除，无非就是，从头还是从尾添加，从头还是从为删除，或者说，先进先出，还是先进后出，也就是队列和栈。</span></p>
<p><span style="line-height: 1.5;">    Linux在这方面设计的比较精巧，而且是一招鲜吃遍天。它的具体实现思想是，我先指定一个全局变量header，你可以选择往这个header的prev方向插也可以选择往这个header的next方向插，嗯，想必你已经明白了，它实现的是一个双向循环链表。对于栈而言就是，每来一个新元素，我就插入到header的next位置，遍历的时候我也往next方向遍历，那么first元素就是最新插入的元素。对于队列而言是，每来一个新元素，我就插入到header的prev位置，遍历的时候仍然是往next方向遍历，但这个时候first元素显然是最老插入的元素。是不是又是前又是后插来插去的把你插糊涂了？这怨不得我，谁让您的思想太邪恶了哪。 于是linux就靠struct list_head这个东西几乎搞定了所有涉及到链表的操作，这也是为什么我说它一招鲜吃遍天的原因。</span></p>
<p><span style="line-height: 1.5;">    <span class="caps">BSD</span> kernel则是实现了list的family， 比如SLIST／LIST／TAILQ等，在这里我说一下有代表性的TAILQ。我们知道BSD就是network的祖师爷，network协议的第一次编码实现就是在BSD系统上。 所以BSD链表的实现也带有明显的网络应用场景痕迹，就像queue.h里面解释的，“因为我们总是会从前往后查找（收包嘛自然是先来的先走了），所以我们双向链表的prev域并不是指向前一个元素，而是指向前一个元素的next域，而前一个元素的next域是指向我这个元素自身”，这样设计的好处从TAIQ_LAST()这个函数的实现里就可以一窥端倪。 BSD并没有像linux那样靠循环链表的表头header就可以想怎么玩就怎么玩，bsd不是一个循环链表。但是我又想实现从头从尾插从头从尾删怎么办，所以它得记录这个链表的first和last元素，这样就可以想从前就从前想从后就从后的插入和删除了。</span></p>
<p><span style="line-height: 1.5;">    说一下TAILQ_LAST()这个函数。TAILQ_LAST()的一个技巧就是，指针就是指针，它不过就是4字节(32bit <span class="caps">CPU</span>)或者8字节(64bit <span class="caps">CPU</span>)而已，和名字无关。 比如我有下面两个结构体：</span></p>
<p><span style="line-height: 1.5;">        struct XX{<br />
</span>            struct XX <strong>first；<br />
<span style="line-height: 1.5;">            struct XX *</strong>last;<br />
</span><span style="line-height: 1.5;">        } header;</span></p>
<p>与<br />
<p style="padding-left: 30px;"><span style="line-height: 1.5;">struct  YY{</span><br />
<span style="line-height: 1.5;">    struct YY <strong>next;</span><br />
<span style="line-height: 1.5;">    struct YY *</strong>prev;</span><br />
<span style="line-height: 1.5;">} type;</span></p><br />
如果我知道了type的next域的地址，我想要获取type的prev元素里面的内容怎么办？ 最直接点的实现就是next域的地址加4再访问就好了，但是这样不优雅，太过于丑陋，作为优雅的BSD，自然要使用其他方法，这也是为什么我说container_of太过于简单粗暴的原因。BSD的实现是，我使用header和type具有相同的内存layout来做文章， 因为next域指向的元素类型跟type一样，那么我把type类型强制转换为header类型，访问header的last域就可以了。说的有点绕，请看代码实现，一看就明白。</p>
<p>&nbsp;</p>
<p><span style="line-height: 1.5;">    总结一下，linux kernel双向链表的基本结构体是，</span></p>
<p><span style="line-height: 1.5;">            struct type {<br />
</span><span style="line-height: 1.5;">                struct type *next;<br />
</span><span style="line-height: 1.5;">                struct type *prev;<br />
</span><span style="line-height: 1.5;">            };</span></p>
<p><span class="caps">BSD</span> kernel双向链表的基本结构体是，</p>
<p><span style="line-height: 1.5;">            struct type {<br />
</span><span style="line-height: 1.5;">                struct type <strong>next;<br />
</span><span style="line-height: 1.5;">                struct type *</strong>prev;<br />
</span><span style="line-height: 1.5;">            };<br />
BSD的实现由应用场景来决定的，Linux Kernel则纯粹是为了一药治百病。 </span></p>
<p>&nbsp;</p>
<p><span style="line-height: 1.5;">    现在已经很晚了，有点疲累，所以匆匆写完，也没有到处找链接给索引。所以只是作为一个入门参考，具体细节您还得自己慢慢分析。另外再说一句，BSD的TAIQ确实设计的很巧妙，不像Linux Kernel的list简单粗暴的用0指针来实现container_of(). 具体细节您得看代码，如有不明白，尽管google，有很多好事者写了这方面的解析，当然大多数不过C+V而已。</span></p>
<p><span style="line-height: 1.5;">    Damn it，从12点10分开始写，已经写到1:30了，得睡觉了。</span></p>
<p><span style="line-height: 1.5;">    明天见：）</span></p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊Kernel：atomic ]]></title>
    <link href="http://laoar.github.io/blogs/361"/>
    <updated>2013-10-16T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/361</id>
    <content type="html"><![CDATA[<p>atomic的意思是原子，那么原子操作该怎么来理解？</p>
<p><a href="http://www.kerneltravel.net/downloads/UnderstandLinuxKernel2nd.pdf">《Understading the Linux Kernel(2nd Section)》</a>的解释是，“操作是单个指令执行，中间不能中断，且避免其他CPU访问同一存储单元”。其实这样说的很不是清楚，会让人费解。我对于原子操作的理解是这样的，说白了，原子操作的目的是，我在对某个地址操作的过程中，这个地址空间里面的数据不会被其他的东西给修改。我们知道，对于CPU而言，它的最小执行单元就是一条指令，指令在流水线（取指／译码／执行等）线执行的过程是不会被打断的，而在两条指令间是可以被打断的，即，程序员的意思是让CPU执行完指令A就去执行指令B，但是CPU在执行完指令A后就可能会被其他东西打断而没有去执行指令B，比如被中断打断；除了被打断，还有可能会被别的CPU给改写数据。原子操作要做的事就是，如何保证在在这些情况下依然可以实现我们前面说的那个目的。</p>
<p>先从atomic的数据结构来说起。看下<a href="http://lxr.oss.org.cn/source/include/linux/types.h?a=mips#L177">linux内核对于atomic type的定义</a>：<br />
<p style="padding-left: 60px;">typedef struct {int <a href="http://lxr.oss.org.cn/ident?a=mips;i=counter">counter</a>; } <a href="http://lxr.oss.org.cn/ident?a=mips;i=atomic_t">atomic_t</a>;<br />
typedef struct {long <a href="http://lxr.oss.org.cn/ident?a=mips;i=counter">counter</a>; } <a href="http://lxr.oss.org.cn/ident?a=mips;i=atomic64_t">atomic64_t</a>;</p><br />
看到这个定义，想必你的第一感一定是，为什么要定义成结构体，而不是直接定义成：<br />
<p style="padding-left: 60px;">typedef int atomic_t: ?</p><br />
这样定义有什么好处吗，我可以定义成int吗？</p>
<p>我们再来看下freebsd对于atomic_t的定义，很遗憾的是，<a href="https://github.com/freebsd/freebsd/blob/master/sys/mips/include/atomic.h">较新的freebsd源码已经看不到atomic_t的定义了，原子操作都是直接使用volatile uin32_t。 </a> 。对于较老版本的freebsd，它是这样定义atomic_t的：<br />
<p style="padding-left: 60px;">typedef volatile int aotmic32_t.<br />
typedef volatile long long atomic64_t;</p><br />
 All right, freebsd和linux确实看起来是两个世界，两帮天才各实现自己的东西，都觉得自己的实现是好的。</p>
<p>我举freebsd对于atomic_t的定义，是为了说明，直接使用typedef int atomic_t不是不可以，一样可以实现要做的事，从语言技巧的角度来看，定义成struct跟不定义成struct没有什么区别。那，为什么linux要定义成一个结构体哪？</p>
<p>再来看下<a href="http://lxr.oss.org.cn/source/Documentation/atomic_ops.txt">linux kernel atomic的maintainer对此的解释</a>：</p>
<p><em>&#8220;The atomic_t type should be defined as a signed integer. Also, it should be made opaque such that any kind of cast to a normal C integer type will fail.  &#8221;</em></p>
<p>他说的倒是很严重的样子，“any kind of cast to a normal C int type will fail”，其实就是说，你丫别随便操作atomic_t这个类型，又是类型转换又是直接操作counter，出了事别怪我丫没有提醒你，你要使用atomic_t 这样类型，就必须得通过我给你提供的一系列atomic interface，我之所以费那么大劲用个结构体来把它给封装起来就是为了防止你乱用。 所以，这句话完全等价于:&#8220;hey, you guy, it is my territery&#8221;. 可以看出，定义成struct代表了该<span style="color: #0000ff;">作者的设计理念，即：一起操作都要通过接口，别走捷径</span>。所以内核开发者切记直接使用atomic_data_p-&gt;counter，这是没有理解作者的设计意图，应该要atomic_read(atomic_data_p)这样来用。</p>
<p>解释完了struct，再来看下volatile。</p>
<p>想必大家都知道，volatile是告诉编译器的指令，让编译器编译的时候别做任何优化，就按照程序员的意图来，不要为了适应机器而改变什么。 该maintainer接下来又解释了为什么没有volatile：</p>
<p><em>Historically, counter has been declared volatile.  This is now discouraged. See Documentation/volatile-considered-harmful.txt for the complete rationale.</em></p>
<p>之所以去掉volatile, Jonathan Corbet先生的解释是,没有必要用volatile,因为它对性能的损害太大了,在必要的地方用barrier()就可以了.  David S. Miller相信了Corbet的话, 正所谓艺高人胆大,于是三下五除二的把atomic.h里面的volatile都给去掉了,然后在必要的地方加了barrier. 不过freebsd guys没有听信Corbet这一套,而依旧很稳妥的在所有的地方都加着volatile.</p>
<p>在这里我解释下volatile和barrier的区别, volatile是用来限制编译器的,它告诉编译器别自作聪明的调整代码或加或减代码来谋取性能,barrier则是限制cpu的,它告诉cpu要按照二进制的指令顺序去执行不要去搞一些乱序来谋求性能. 二者的杀伤力自然不一样, volatile显然更威猛一些.  当然barrier也细分了一些小弟来干不同的事,毕竟它依然对性能有很大的伤害,比如有些是防止指令乱序的,有些是防止对内存访问乱序的.</p>
<p>说完了数据结构，接着看下具体的操作。以mips为例来说明，感觉mips设计的挺优雅的：）</p>
<p>我们知道，一条指令是不会被打断的，自然对于一条指令就能够搞定的事情，我们是没有必要费劲去考虑它的原子性的，比如读和写，都可以用一条指令来完成，所以atomic_read和atomic_write会写成这个样子：<br />
<p style="padding-left: 60px;">#define atomic_read(v) ((v)-&gt;counter)<br />
#define atomic_write(v, i) ((v)-&gt;counter = i)</p><br />
搞成这个样子，就是我前面说的，这是作者的理念：一切操作通过接口。</p>
<p>&nbsp;</p>
<p>MIPS是RISC架构, RISC架构都是load/store Architecture,  即它对于内存的读是通过load，写是通过strore。显然，对于这种架构，是不能够在内存地址间传递数据的，必须要通过寄存器来进行，那么指令的操作数显然不能包含两个不同的地址。比如我给某个地址里面的值加1， 那就得首先把这个地址里面的数据load到寄存器，然后给寄存器加1，再把该值store到那个内存地址。 于是问题就来了，这个add操作涉及到3条指令，在指令间就可能会被中断打断，也可能在store之前被其他的cpu给抢先一步改了这个内存里面的值。于是，就有了mips的<a href="http://en.wikipedia.org/wiki/Load-link/store-conditional">ll/sc</a>这个指令组合，ll/sc这个指令组合通过下面这段代码来保证对内存访问的原子性：<br />
<p style="padding-left: 60px;">1:<br />
ll rt, offset(base)<br />
addi rt, rt, 1<br />
sc rt, offset(base)<br />
beqz rt, 1b</p><br />
    首先来解释下，它是怎么处理被中断打断的情况的。</p>
<p>mips为了处理ll/sc指令被中断打断的情况，专门搞了一个flag，叫做<a href="http://www.cs.cornell.edu/courses/cs3410/2013sp/MIPS_Vol2.pdf">LLbit</a>，这个flag对于程序员是不可见的，即程序员无法操作该flag。 当执行ll指令的时候，cpu会把LLbit给置为1,然后在执行sc指令的时候如果该bit依然为1,就认为操作的这个内存空间没有被改写，然后store新值进去，如果sc的时候发现该bit变为了0,那么就认为操作的内存空间在ll执行后sc执行前被改写了，此时sc就不往该内存空间更新新值。 中断返回时会把该bit给清零，由此来确保如果ll/sc之间被中断打断，那么就要重新ll（beqz指令就是做这个事的）。</p>
<p>然后来看下它是怎么处理多处理同时操作同一个内存地址的情况的。</p>
<p>为了处理这种情况，mips又专门搞了个寄存器，叫做<a href="http://www.cs.cornell.edu/courses/cs3410/2013sp/MIPS_Vol3.pdf">LLAddr</a>，它是一个协处理器的寄存器。当cpu执行ll指令的时候，会把ll的地址给保存在自己的协处理器寄存器LLAddr里面，然后cpu就会监控该地址，看是否在ll/sc之间别的处理器会写该地址，如果别的cpu写了该地址，就把LLbit给清零，于是接下来sc失败（即不往内存地址store数据，同时赋rt为0），再去重新ll（beqz跳转到前面的ll）。 Well，问题来了，假如两个cpu都执行ll/sc, 而且是操作的同一个内存地址，会出现什么情况？ 我们知道，在ll/sc之间的指令是不会去写这个内存地址的（否则，还要sc干嘛），那么总有一个cpu会首先执行到sc，第一个执行sc指令的cpu会成功的stroe进去新值，另外一个cpu发现自己LLAddr被别人改写了，于是将自己的LLbit置为0,那么sc失败，重新来过。你也许还会继续钻牛角尖，这俩CPU赶巧了，同时执行sc了，那会发生什么？很遗憾的是，mips manual里没有说这种情况，只说了这么一句话：</p>
<p><em>“Executing LL on one processor does not cause an action that, by itself, causes an SC for the same block to fail on another processor.”</em></p>
<p>这句话的意思就是我前面说的，两个cpu同时ll/sc一个内存地址，sc的时候会让另外一个cpu的sc失败，而自己sc成功。我找遍了各种资料，也没有见详细说明两个cpu同时ll/sc同一个内存地址时，sc指令又碰巧同时执行了，到底会发生什么。我觉得，我们可以这样想，就像世界杯踢点球一样，要是俩决赛的队伍到了最后要踢点球了，而每一个球员又碰巧都罚进了，那岂不是这俩队就一直射到世界末日？Hmm… 相信我，总有人会成为倒霉蛋的。球场上不是有裁判么，CPU也有类似的决策：总线裁决。这种情况下就要靠总线裁决。</p>
<p>&nbsp;</p>
<p>P.S.: 写这篇blog，花了一些精力在找各种链接上，anyway， 治学要严谨。 我之前的领域是linux kernel，从未看过freebsd，现在一对比freebsd，发现这两者的区别确实很有趣。感觉这就像为什么很多人喜欢旅游一样，虽然旅游归来又继续朝九晚五的按步就班，看似没有任何改变，但终究两个地域的对比会对你有所影响，只是没有那么明显而已，这是一个量变到质变的过程，没有人会一下子成为千万富翁的，除非你去抢银行。 所以，我们在一个地方呆久了，就应该出去走一走，看看外面是什么样子。在一个公司或者一个领域也是这样，抬起头来看看外面，否则就像世外桃源似的，乃不知有汉，无论魏晋。不过这样未尝不是件好事，北朝鲜的人民生活就很幸福，谁说不是哪。关键是，就像李国柱大牛（注：华为公司一位德高望重的前辈）说的，看你自己追求的是什么。</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊Kernel：该怎么理解Terminal]]></title>
    <link href="http://laoar.github.io/blogs/338"/>
    <updated>2013-09-28T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/338</id>
    <content type="html"><![CDATA[<p style="padding-left: 30px;"></p>
<p>    对于unix系的操作系统而言，即使是看起来很酷的MacOS，都有Terminal这个东西：</p>
<p><a href="http://www.laoar.net/wp-content/uploads/2013/09/b6a1a15ff43df3648ff3883c28a223e0.png"><img class="aligncenter size-full wp-image-339" title="b6a1a15ff43df3648ff3883c28a223e0" src="http://www.laoar.net/wp-content/uploads/2013/09/b6a1a15ff43df3648ff3883c28a223e0.png" alt="" width="172" height="147" /></a>那么Terminal到底是什么，该怎么来理解它？<br />
<p style="padding-left: 30px;"></p><br />
    Terminal是一种设备，它具备输入输出能力，通常我们在程序里会使用reguler files，pipe，socket来用作输入输出，但是他们远不具备terminal那么强大的输入输出能力。 Terminal设计的目的是为了让我们与电脑的交互更容易。<br />
<p style="padding-left: 30px;"></p><br />
    当你在Terminal里运行一个程序的时候，你可以通过Ctrl-C来给它发送一个interrrupt信号（SIGINT）；如果该程序是从文件里面读的\x03字符，显然它不会收到SIGINT信号。当你在Terminal里运行一个程序，如果它产生了一个^G字符(\x07)，那么，你就会听到一个beep声；如果该程序是从文件里面读取到的该字符，或者将该字符写入到一个文件中，显然不会产生beep声。举个例子，在终端里面运行下面这段代码，你会不停的听到beep声。Yeah,it is so easy,but so funny.And, don&#8217;t forget to ask why.<br />
<p style="text-align: left; padding-left: 60px;"><span style="color: #808000;">#include &lt;stdio.h&gt;</span><br />
<span style="color: #808000;">#include &lt;unistd.h&gt;</span></p><br />
<p style="text-align: left; padding-left: 60px;"><span style="color: #808000;">int main()</span><br />
<span style="color: #808000;">{</span><br />
<span style="color: #808000;">    unsigned int usec = 100;</span><br />
<span style="color: #808000;">    while (1) {</span><br />
<span style="color: #808000;">        printf(&#8220;\x07&#8221;);</span><br />
<span style="color: #808000;">        usleep(usec);</span><br />
<span style="color: #808000;">    }<br />
</span><span style="color: #808000;">    return 0;<br />
</span><span style="color: #808000;">}</span></p><br />
    当你在终端里执行<span style="color: #999999;">ls -G</span>(至少Mac是这样，FreeBSD也这样，SUSE也这样，例外的是Redhat，它是<span style="color: #999999;">ls &#8212;color</span>),你会看到当前目录里的文件和子目录会有颜色的显示。不过，如果该目录里的东西要被写入到一个文件中，well，你不能在一个文本文件中显示出颜色来，can U now？<br />
<p style="padding-left: 30px;"></p><br />
    好吧，这到底是怎么一回事，它是怎么个工作原理哪？ 当程序与一个文件交互时，最终会通过kernel driver。如果你在一个ext2分区上open/read/write，ext2 filesystem的drivder就会被调用；如果你open一个FIFO，它会使用pipe driver(Linux:fs/pipe.c;  FreeBSD:<span style="color: #ff0000;"><span class="caps">TBD</span></span>); 如果你open一个terminal设备节点，terminal设备驱动就会处理这个请求。<br />
<p style="padding-left: 30px;"></p><br />
    Terminal driver控制的设备稍微多一些。 ext2 driver只需要和磁盘进行通信；pipe driver则不涉及任何硬件；而terminal driver需要监控keyboard／mouse，将字符打印到屏幕上，以及将beep声发送到speaker。从这个角度来看，terminal可以认为是个抽象化的东西，它代表了人机交互所有用到的东西。当然，X系统也是运行在一个terminal上，对linux系统而言，它对应的设备是/dev/tty7, 这也是为什么使用Alt+Ctrl+F7可以切换到X系统的原因。<br />
<p style="padding-left: 30px;"></p><br />
    对于一个典型的PC而言，在任何时间，某一个终端会运行在前台，其他的终端则是在后台。 位于前台的终端会接收keyboard／mouse的输入，同样也会独占的访问在monitor上显示的东西。比如，当你按下某个键比如‘K’，terminal driver会将该字符显示到屏幕上，并且将它存在一个buffer里面，因此，当接下来某个进程从终端设备来读取数据的时候会接收到这个字符。 terminal也会对一些功能键作出反应，比如Ctrl-A，terminal将其显示为^A,并且在process获取它之前将其编码为\x01。当一个process往terminal上写字符时，这些字符会被显示到屏幕上;但对于一些控制序列，比如以<a href="http://www.gnu.org/software/screen/manual/html_node/Control-Sequences.html">ESC开始的control sequences</a> ，terminal不会将其显示到屏幕上，而是会作出一些特殊的行为，比如移动cursor，或者改变当前的文本颜色，等等，当然process是不能直接看到这些行为的，它是为了给你看的。另外，BEL字符会产生一个beep音，而不是通常的可视化输出。<br />
<p style="padding-left: 30px;"></p><br />
     Terminal也会用来控制任务，比如，Ctrl-C来打断一个process group，Ctrl-Z来挂起一个process group，对于terminal driver而言就是发动SIGINT或者SIGSTP给在前台运行的process group。 任务控制是基于这个原理：运行在一个控制终端（/dev/tty）上的process通常是受该控制终端控制的。例如，terminal driver会纪录它控制了哪些session以及哪个process group在前台。/dev/tty是指向控制终端的一种特殊设备，具体是指向哪个设备可以通过它提供的ioctl接口来获取。<br />
<p style="padding-left: 30px;"></p><br />
    所有前面说到的这些很cool的功能都是在terminal driver里面来实现的，可以概括为下图所示：</p>
<p><a href="http://www.laoar.net/wp-content/uploads/2013/09/1e5e32cb531e354b6f897853e0c49280.png"><img class="aligncenter size-full wp-image-340" title="1e5e32cb531e354b6f897853e0c49280" src="http://www.laoar.net/wp-content/uploads/2013/09/1e5e32cb531e354b6f897853e0c49280.png" alt="" width="732" height="576" /></a><br />
<p style="padding-left: 30px;"></p><br />
    与Terminal相关容易把人搞糊涂的一个概念是shell。shell是一个应用程序，它用来提示你输入，并且执行你的命令。shell是不感知到字符显示以及处理按键事件的，这是terminal要为它处理好的，因此terminal是对shell的一个封装，给它提供了一个人机交互的桥梁。 通常我们使用较多的shell是bash。 对应到上图，“foregroud process group”基本上都会有一个shell进程，它要么在读写字符，要么在等待它的子进程（这个子进程在读或者写字符）退出。 这个前台进程组的&quot;stdin&quot;会得到控制终端发送给它的字符，这些字符可能是你输入的，也可能是程序产生的。 这个前台进程组的“stdout”是控制终端输入以显示到屏幕上的ASCII码流。<br />
<p style="padding-left: 30px;"></p><br />
    还有个东西叫做console(/dev/console), console是terminal的一种类型。 对于我们而言，console比较直观的一个理解是，它是系统日志输出的地方，那么从这个角度看，console可以认为是用来管理系统的。</p>
<p><strong><span style="color: #ff0000;"> P.S.：</span></strong></p>
<p><strong><span style="color: #ff0000;">个人见解，如果觉得写的有不对的地方，请直接喷：）</span></strong></p>
<p>&nbsp;</p>
<p>ref：</p>
<p><sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup> <a href="http://www.linusakesson.net/programming/tty/index.php">The <span class="caps">TTY</span> demystified </a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊Kernel：从Linux到FreeBSD]]></title>
    <link href="http://laoar.github.io/blogs/325"/>
    <updated>2013-09-21T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/325</id>
    <content type="html"><![CDATA[<p><em><strong><span style="color: #0000ff;">今晚恰好是中国好声音那英组的决赛，看完比赛后就一直听姚贝娜的《Dear Friend》，经过了无数个单曲循环后写完了这篇日志。姚贝娜唱的太好听了。</span></strong></em></p>
<p>总的来说，FreeBSD Kernel比Linux Kernel要简洁的多。我感觉可能是以下两个原因：1）linux考虑的场景多，所实现的功能也更多一些，所以linux比freebsd在编译时的配置要灵活。2）FreeBSD是个中央集权式的组织结构，linux则是民主式组织结构，所以freebsd的很多改进都是有组织有计划，代码看起来就很清晰，而linux是自底向上的改进，代码看起来就散乱些。</p>
<p>下面来看下linux kernel 和freebsd kernel在代码上的主要差异：<br />
<h2><strong>1. 编译</strong></h2><br />
Linux和FreeBSD都是使用Makefile来管理整个工程的。 二者在编译上最大的差异是configuration的不同。</p>
<p>Linux为了灵活配置，在内核编译配置上使用了Kconfig机制，使用该机制能够很方便的选择编译哪些功能。Kconfig也是使用Makefile来控制的，使用make menuconfig这个命令。</p>
<p>FreeBSD则是使用了一个专门的工具config来配置需要编译的内核代码。<br />
<h2><strong>2. 内核启动</strong></h2><br />
bootstrap之后是OS的启动，这对于所有的操作系统都是一致的。</p>
<p>二者在启动过程主要是如下的不同。</p>
<p><em>1）</em>linux内核使用了镜像压缩机制，在从bootloarder跳转到内核后先执行的是一个压缩的镜像，然后进行自解压。FreeBSD则没有这个压缩机制。</p>
<p><em>2）</em>linux模块的初始化是分层级进行，每个优先级都对应于一个section。linux首先初始化高优先级的initcall，再去初始化低优先级的initcall，在某个优先级内，比如A模块和B模块都属于优先级1,那么A模块和B模块的初始化顺序则是由链接过程决定的，哪个模块先被链接到section的前面，自然要先执行哪个模块。 FreeBSD的模块初始化是使用的SYSINIT，SYSINIT会在编译时首先定义好各个模块的初始化顺序，然后内核就按照这个顺序来初始化各个模块。</p>
<p><em>3）</em>swapper／init／idle线程</p>
<p>不同在于SMP系统。linux内核的每个CPU都有自己的swapper线程（0号线程），init线程（1号线程）则只有主核有，对于linux而言，swapper线程就是idle线程，即在CPU空闲的时候会去唤醒idle线程。 FreeBSD的内核swapper线程和idle线程是不同的线程，只有主核有swapper线程（0核线程）和init线程（1号线程），每个核都有自己的idle线程，在CPU空闲的时候会唤醒该idle线程。<br />
<h2><strong>3.  Task Management 和 Scheduling Algorithm</strong></h2></p>

		<ol>
			<li>1）主要数据结构<br />
Linux kernel使用task_struct这个结构体(include/linux/sched.h)来描述进程, 使用thread_info结构体（arch/arm/include/asm/thread_info.h）来描述线程状态信息,thread_info是和architecture强相关的，所以定义在了arch目录下。调度是以task_struct为基本单位来进行的，每个线程／进程都有自己的task_struct,线程和进程的区别主要在于，进程有自己独立地址空间，而线程则是共享进程的地址空间。</li>
		</ol><p>FreeBSD使用proc这个结构体（sys/sys/proc.h）来描述进程,使用thread结构体（sys/sys/proc.h）来描述线程信息，FreeBSD的调度单位是thread，对于单线程的进程而言，它有一个proc结构体和一个thread结构体，对于多线程的进程而言，则是每个线程都有自己的thread结构体，多线程共用一个proc结构体。 线程和进程的主要区别也是线程没有自己的独立地址空间。</p>

		<ol>
			<li>2）调度机制<br />
Linux的进程分为实时进程和普通进程两种，实时进程有FIFO和RR两种调度策略， 普通进程则是CFS调度算法。 每个CPU都维护着自己的运行队列／等待队列链表。</li>
		</ol><p>FreeBSD则是将进程分为以下几种调度类型：ITHD／KERN／REALTIME／TIMESHARE／IDLE，从左往右优先级递减。其中，ITHD是针对中断下半部的线程（FreeBSD用线程来处理中断，这是和linux的一个很大的区别），所以优先级最高；接着是内核线程；然后是实时用户态线程；接着是分时线程；CPU空闲的时候再唤醒idle线程。FreeBSD有三种调度算法：实时调度，针对实时进程；分时调度，针对普通进程；ULE调度算法，针对SMP系统的负载均衡。 每个CPU都维护着自己的idle队列／curren队列／next队列这三个链表。<br />
<h2><strong>4. Memory Management</strong></h2></p>

		<ol>
			<li>1）主要数据结构<br />
Linux kernel使用mm_struct这个结构体（include/linux/mm_types.h）来描述进程的地址空间。</li>
		</ol><p>FreeBSD使用vmspace这个结构体（sys/vm/vm_map.h）来描述进程的地址空间。</p>

		<ol>
			<li>2)  内存管理策略<br />
二者并无明显的差异。都是分页机制＋访问权限控制。<br />
<h2><strong>5. Filesystem</strong></h2><br />
都是使用的VFS机制。差异在于具体文件系统上。</li>
		</ol><p>linux kernel支持磁盘文件系统（EXT系列），块设备文件系统（yaffs2／jffs2）, 网络文件系统（NFS），虚拟文件系统（pseudo filesystem，例如procfs／devfs） 。</p>
<p>FreeBSD在具体实现上跟linux略有不同，FreeBSD的具体存储机制是Filestore，filestore有三种管理方式：1）针对块设备：FFS， 2）针对内存：MFS， 3）日志文件系统。FreeBSD也支持NFS。FreeBSD也支持虚拟文件系统，略有不同的是，FreeBSD的procfs是指process filesystem，就类似于内核提供给用户态一个sysctl接口来查看进程信息。<br />
<h2><strong>6.  Interrupt</strong></h2><br />
FreeBSD的中断下半部对应于一个内核线程，是由内核线程来处理中断服务程序。Linux的中断下半部则没有所谓的线程，直接由中断向量表进入中断中断服务程序。<br />
<h2><strong>7. Sync Mechanism</strong></h2><br />
二者的同步机制基本一样，都是spinlock／mutex／semaphore这些机制以及衍生。<br />
<h2><strong>8. Debug </strong></h2></p>

		<ol>
			<li>1）用户态<br />
linux和FreeBSD都支持GDB，GDB的实现在两个OS上也是一致的，都是通过ptrace系统调用。</li>
		</ol><p>linux有个很强大的工具strace来跟踪系统调用，FreeBSD上没有该利器，不过FreeBSD可以使用ktrace＋kdump来实现strace的功能。</p>

		<ol>
			<li>2）内核态调试<br />
linux的kgdb可以分析crash dump，也可以对内核进行live debug。</li>
		</ol><p>FreeBSD则是使用kgdb来分析crash dump，然后使用ddb工具来进行 live debug。</p>

		<ol>
			<li>3）其他杂项调试工具<br />
大同小异，半斤八两。</li>
		</ol><p>&nbsp;</p>
<p>P.S.:对FreeBSD代码看的还不多，很多地方也许写的不对。后续会继续完善。</p>
<p><span style="color: #ff0000;"><strong> TBD</strong></span></p>
<p>&nbsp;</p>
<p>ref：</p>
<p><a title="linux kernel" href="http://http://www.amazon.cn/Understanding-the-Linux-Kernel-Bovet-Daniel-P/dp/0596005652/ref=sr_1_1?ie=UTF8&amp;qid=1379698958&amp;sr=8-1&amp;keywords=understanding+linux+kernel" target="_blank"><sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup> Understanding the Linux Kernel</a></p>
<p><a title="FreeBSD Kernel" href="http://www.amazon.cn/FreeBSD%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0-%E9%BA%A6%E5%8D%A1%E6%80%9D%E5%85%8B/dp/B0012X9LD6/ref=sr_1_1?ie=UTF8&amp;qid=1379699151&amp;sr=8-1&amp;keywords=freebsd" target="_blank"><sup class="footnote" id="fnr2"><a href="#fn2">2</a></sup>The Design and Implementation of the FreeBSD Operation System</a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在高铁上聊Kernel：what is the fucking ABI(程序二进制接口)?]]></title>
    <link href="http://laoar.github.io/blogs/316"/>
    <updated>2013-09-02T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/316</id>
    <content type="html"><![CDATA[<p>P.S.:在高铁上写完的这篇文章。</p>
<p>在定义ABI之前，先看下API，对于API我们更熟悉一些。 二者的对比会帮助我们来理解ABI。</p>
<p>API是Application Programming Interface的缩写，它的意思是程序编程接口。 一个API是不同代码片段的连接纽带。它定义了一个函数的参数，函数的返回值，以及一些属性比如继承是否被允许。 因此API是用来约束编译器的：一个API是给编译器的一些指令，它规定了源代码可以做以及不可以做哪些事。在说到API的时候，也会涉及到函数的条件，行为以及出错环境。从这个角度看， 一个API也可以看作是被人使用：一个API是给程序员的一些指令，它规定了函数需要什么以及会做什么。</p>
<p>ABI是Application Binary Interface的缩写，它的意思是程序二进制接口。 一个ABI是不同二进制片段的连接纽带。 它定义了函数被调用的规则：参数在调用者和被调用者之间如何传递，返回值怎么提供给调用者，库函数怎么被应用，以及程序怎么被加载到内存。 因此ABI是用来约束链接器的：一个ABI是无关的代码如何在一起工作的规则。 一个ABI也是不同进程如何在一个系统中共存的规则。 举例来说，在Linux系统中，一个ABI可能定义信号如何被执行，进程如何调用syscall，使用大端还是小端，以及栈如何增长。从这个角度看，一个API是用来约束在一个特定架构上操作系统的一系列规则，</p>
<p>举个例子，在ARM 32bit架构上，对于如下代码片段而言：<br />
<span style="color: #993300;">    void foo_a(void)</span><br />
<span style="color: #993300;">    {</span><br />
<span style="color: #993300;">        int a<sup class="footnote" id="fnr10"><a href="#fn10">10</a></sup>;</span><br />
<span style="color: #993300;">        int val;</span><br />
<span style="color: #993300;">        val = foo_b(a<sup class="footnote" id="fnr0"><a href="#fn0">0</a></sup>, a<sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup>, a<sup class="footnote" id="fnr2"><a href="#fn2">2</a></sup>, a<sup class="footnote" id="fnr3"><a href="#fn3">3</a></sup>, a<sup class="footnote" id="fnr4"><a href="#fn4">4</a></sup>, a<sup class="footnote" id="fnr5"><a href="#fn5">5</a></sup>, a<sup class="footnote" id="fnr6"><a href="#fn6">6</a></sup>, a<sup class="footnote" id="fnr7"><a href="#fn7">7</a></sup>, a<sup class="footnote" id="fnr8"><a href="#fn8">8</a></sup>, a<sup class="footnote" id="fnr9"><a href="#fn9">9</a></sup>);</span><br />
<span style="color: #993300;">    }</span><br />
那么，参数a<sup class="footnote" id="fnr0"><a href="#fn0">0</a></sup>~a<sup class="footnote" id="fnr3"><a href="#fn3">3</a></sup>会被加载到r0～r3这四个寄存器中，参数a<sup class="footnote" id="fnr4"><a href="#fn4">4</a></sup>~a<sup class="footnote" id="fnr9"><a href="#fn9">9</a></sup>则会压入栈中，且压栈方向是从右至左，即a<sup class="footnote" id="fnr9"><a href="#fn9">9</a></sup>先入栈，a<sup class="footnote" id="fnr4"><a href="#fn4">4</a></sup>最后入栈。foo_b的返回值则会放在r0寄存器中。这就是ABI所约束的规则。</p>
<p>一个ABI是被kernel／toolchain／架构这三驾马车共同定义的。这三者每一个都必须遵守它。一般，架构规划好标准的ABI，然后操作系统或多或少的采用这些标准，这些细节都会在架构手册里面文档化。</p>
<p>Ref:</p>
<p><sup class="footnote" id="fnr1"><a href="#fn1">1</a></sup> <a title="3 Volume Set of Intel® 64 and IA-32 Architectures Software Developer’s Manuals" href="http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html ">3 Volume Set of Intel® 64 and IA-32 Architectures Software Developer’s Manuals</a></p>
<p><sup class="footnote" id="fnr2"><a href="#fn2">2</a></sup> <a title="Application Binary Interface for   the ARM Architecture" href="http://infocenter.arm.com/help/topic/com.arm.doc.ihi0036b/IHI0036B_bsabi.pdf">Application Binary Interface for the ARM Architecture</a></p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊Kernel：Busy Waiting or Sleeping？]]></title>
    <link href="http://laoar.github.io/blogs/299"/>
    <updated>2013-08-31T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/299</id>
    <content type="html"><![CDATA[<p style="padding-left: 30px;"></p>
<p>     在某些情况下，我们需要将当前线程的执行挂起，直至某一事件发生。这个事件可能是一个有争抢的资源变为可用，时间的流逝，或者锁的释放。这个等待可以通过两种基本的方式来实现：Busy Waiting 和Sleeping。<br />
<p style="padding-left: 30px;"></p><br />
    系统通过在循环里面旋转并且不停的去检查问题事件是否发生来实现Busy Waiting。   比如，如果我们想要等待5s，可以这样来实现：<br />
<p style="padding-left: 30px;"><span style="color: #993300;">const int until = get_time() + 5;</span><br />
<span style="color: #993300;">while(until &gt; get_time())</span><br />
<span style="color: #993300;">;</span></p><br />
这样做的好处是，它的实现较简单，并且如果不等太久它在性能上会表现很好，因为避免了切换到其他线程的开销。坏处也很明显：把CPU cycle浪费在执行无任何价值的东西上。<br />
<p style="padding-left: 30px;"></p><br />
    Busy Waiting相对于Sleeping比较容易理解。Sleeping通过更复杂的方式来实现：首先构造一个需要等待的线程的链表，叫作等待队列；接着把自己加入到等待队列中，控制权交给内核；然后当问题事件发生时让内核唤醒该链表里的一个（些）进程去执行。举例来说，你可能需要在某一个mutex变得可用时让内核唤醒你，于是把控制权交给内核，让内核去执行除你之外的进程，在该mutex变得可用时内核再去唤醒你。如下：<br />
<p style="padding-left: 30px;"><span style="color: #993300;">mutex_lock(&amp;m_lock);</span><br />
<span style="color: #993300;">    list_add_tail(current, &amp;m_lock-&gt;wait_list);</span><br />
<span style="color: #993300;">    schedule();</span></p><br />
<p style="padding-left: 30px;"><span style="color: #993300;">mutex_unlock(&amp;m_lock);</span><br />
<span style="color: #993300;">    wake_up_process(m_lock.wait_list.next-&gt;task);</span></p><br />
     Sleeping相对于Busy Waiting的好处是，内核可以在等待期间去执行有意义的事情。坏处则是它的开销：维护一个链表，让你自己去睡眠，上下文切换到一个新的进程（再加上切换回你自己）。如果等待的时间较短，显然使用Busy Waiting更合理。</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊Kernel：do{…}while(0)]]></title>
    <link href="http://laoar.github.io/blogs/289"/>
    <updated>2013-08-28T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/289</id>
    <content type="html"><![CDATA[<p>在内核代码里我们会看到很多do{…}while(0)来定义的宏，比如(摘自include/asm-generic/barrier.h)：<br />
<img src="http://laoar.github.io/images/289.jpg"><br />
linux kernel里这么定义自然有其特殊的目的。</p>
<p>在C语言里，使用do/while(0)模式来定义的宏在任何情况下都有同样的行为，而且在C语言里面，只有do/while(0)模式来定义的宏才会在任何情况下(比如，在没有“ }”的if语句中)都有同样的行为.</p>
<p>举一些例子。</p>
<p><span style="line-height: 1.5;">     <span style="color: #993300;">＃define foo(x)  a(x); b(x)</span></span></p>
<p>对于 <span style="color: #993300;">foo(test);<br />
</span>会被展开为： <span style="color: #993300;">a(test); b(test);</span></p>
<p>&nbsp;</p>
<p>如果是这样的话，这看起来很正确，没有任何错误，是你原本想要得到的结果。</p>
<p>但如果这样用：<br />
<span style="color: #993300;">     if (cond)<br />
</span><span style="color: #993300;">         foo(test);<br />
</span>    它就会被展开为：<br />
<span style="color: #993300;">    if (cond)</span><br />
<span style="color: #993300;">        a(test); b(test);<br />
</span>    它的行为就变成了：<br />
<span style="color: #993300;">    if (cond)          </span><br />
<span style="color: #993300;">        a(test); </span><br />
<span style="color: #993300;">     b(test);<br />
</span>    这并不是你原本想要的。</p>
<p>然后，我们重新定义该宏，使用do/while(0)来封装：<br />
<span style="color: #993300;"> ＃define foo(x) do{a(x); b(x)}while(0)</span></p>
<p>这样定义的宏和前面定义的宏具有同样的作用，只不过，do确保了它的整个逻辑都在大括号里面执行，while(0)确保它只执行一次。所以它跟没有do/while(0) 的宏具有同样的效果。那么，他们有什么不同吗？让我们再来看前面那个例子：<br />
<span style="color: #993300;">    if (cond)          </span><br />
<span style="color: #993300;">        foo(test);<br />
</span>    现在它就变成了：<br />
<span style="color: #993300;">    if(cond)</span><br />
<span style="color: #993300;">        do{a(test); b(test)}while(0);<br />
</span>    实际上就是：<br />
<span style="color: #993300;">    if(cond){</span><br />
<span style="color: #993300;">        a(test);</span><br />
<span style="color: #993300;">        b(test);</span><br />
<span style="color: #993300;">    }</span></p>
<p>你可能会疑问，为什么不直接使用“{}”来定义该宏？让我们来看下面这种情况：<br />
<span style="color: #993300;">#define foo(x) {a(x); b(x);}</span><br />
那么：<br />
<span style="color: #993300;">    if(cond)</span><br />
<span style="color: #993300;">        foo(test);</span><br />
<span style="color: #993300;">    else</span><br />
<span style="color: #993300;">        bin(test);</span><br />
就变成了：<br />
<span style="color: #993300;">    if(cond){</span><br />
<span style="color: #993300;">        a(test);</span><br />
<span style="color: #993300;">        b(test);</span><br />
<span style="color: #993300;">    };</span><br />
<span style="color: #993300;">    else</span><br />
<span style="color: #993300;">        bin(test);<br />
</span>    显然这是一个语法错误，因为else前面没有if。</p>
<p>使用这种方式来定义宏，还有个好处是，在&quot;{&#8230;}&quot;里面可以定义自己的局部变量，而不被外面的变量所干扰。</p>
<p>在linux内核代码里，很多宏都是使用do/while(0)来封装的，目的就是为了让它在任何情况下都具有同样的行为。在我们自己的代码里，也要养成这种好习惯。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊kernel：调度入门]]></title>
    <link href="http://laoar.github.io/blogs/269"/>
    <updated>2013-07-29T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/269</id>
    <content type="html"><![CDATA[<p>前言</p>
<p>前面说了内存管理入门， 内存管理是理解linux内核的基础， 如果不了解内存管理，就无法深入了解内核，理解了内存管理，再学习内核的其他部分就会相对很容易。</p>
<p>这次我要说的是调度这部分， 调度在嵌入式系统中也是一个很重要的方面，众所周知，嵌入式系统对实时性要求较高， 实时性本身就是一个调度问题。</p>
<p>&nbsp;</p>
<p><strong>一 进程调度的目的</strong></p>
<p>linux进程调度是为了支持多进程，如果只是单一进程，显然是不需要调度的。 在对多进程的支持过程中， 如何合理的分配系统资源是进程调度要解决的问题，主要包括cpu该选择哪一个进程执行以及让该进程一次执行多长时间，另外一个就是smp系统的负载均衡， 即如何提高多核系统的并行执行性能。</p>
<p>进程调度是针对task_running状态的进程进行调度， 如果进程不处于task_running状态，进程调度跟它就是没有关系的。 于是就引入了进程状态这个概念。</p>
<p><strong>二 进程状态</strong></p>
<p>进程状态分为task_running状态／睡眠状态／task_stop状态／僵死状态。</p>
<p>task_running状态就是可以被调度的状态，内核在调度点都是选择运行队列上的一个进程来执行。</p>
<p>睡眠状态分为两种：不可中断状态（R状态）和可中断状态（S状态），一般是进程在运行状态想获得某一个资源但是暂时又得不到，那么该进程就会进入睡眠状态，在条件成立的时候再唤醒该进程，唤醒的方式一般都是通过wake_up系列函数来唤醒。 对于S状态的进程，它还可以被信号给唤醒。</p>
<p>task_stop状态是指正在运行的状态收到了sig_stop等一些信号而进入暂停状态， 这可以通过sig_continue信号来将其唤醒。</p>
<p>僵死状态： 当进程已经停止运行，但是其父进程还没有读取该子进程的exit状态时，它的task_struct就会驻留在内存中形成了僵死进程。僵死进程的解决方法是：一是让父进程来waitpid来接管sigchld信号，二是 结束它的父进程，让init进程来处理。 产生的原因：如果父进程fork子进程时没有处理sigchld这个信号， 就会形成僵死进程,有时候可能网络原因等也会产生僵死进程。</p>
<p><strong>三 进程调度点</strong></p>
<p>进程有用户态和内核态这两个状态， 即它有两个栈空间，分别时用户栈空间和内核栈空间。 在内核态下和用户态下都可以发生调度，内核态的调度发生在从中断上下文返回到内核态之前（即进程被中断打断了，中断处理完要返回的时候）， 用户态的调度发生在从内核态返回到用户态的时候（即从系统调用返回的时候。）。  内核态的调度需要打开内核抢占，如果不支持内核抢占，在内核态是不会发生调度的。</p>
<p>ok， 接下来要搞清这个问题。假如我一个进程正在处于用户态，突然来了个中断，会发生什么？ 这个时候，cpu会从特权级3进入特权级0, 进程会由用户态切换到内核态（即从用户态的栈切换到内核态的栈），用户态堆栈指针会压入内核堆栈，中断执行完，首先返回到内核态，然后在从内核态返回到用户态的时候会恢复用户态堆栈。</p>
<p><strong>四 进程的调度策略</strong></p>
<p>进程按照其优先级分为实时进程和普通进程。</p>
<p>实时进程的优先级较高， 实时进程又有两种调度策略：一个时FIFO，另一个是RR。对于FIFO进程而言，只有它执行完，才会选择其他进程执行（没有流控的情况下），而对于RR进程， 则是时间片轮转策略。</p>
<p><strong>五 抢占</strong></p>
<p>linux内核抢占需要在menuconfig里面打开config_preempt选项。</p>
<p>用户抢占则是天生就可以的， 如果不可以， 那怎么支持多进程涅：）</p>
<p><strong>六 从用户态进入内核态的情况</strong></p>
<p>1. 系统调用</p>
<p>2. 异常指令（其实系统调用也是一个异常指令）</p>
<p>3. 中断</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊kernel：内存管理入门]]></title>
    <link href="http://laoar.github.io/blogs/266"/>
    <updated>2013-07-29T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/266</id>
    <content type="html"><![CDATA[<p style="text-align: left;">前言</p>
<p style="text-align: left;">最近稍有闲暇将自己的一些所学做了整理， 只是浅显的说个大概，没有往细了说，观题目就可知，仅作入门：）</p>
<p><strong>1. 内存管理的目的</strong></p>
<p>由于计算机系统所含的实际物理内存大小是有限的， 所以CPU中通常都提供了内存管理机制来对系统中的内存进行有效的管理。 再intel CPU中， 提供了两种内存管理机制： 分段机制 和 分页机制。</p>
<p>分页管理系统是可以选择的， 它有menuconfig的配置来决定是否需要分页。</p>
<p>对于linux内核而言，它并没有使用intel CPU的分段机制， 而只是采用了它的分页机制。</p>
<p>内存管理的一个目的是地址变换，另外一个目的是寻址保护。</p>
<p><strong>2. 分页机制主要明白以下概念：</strong></p>
<p>虚拟地址，物理地址， 页，page 结构体。</p>
<p>对于进程而言， 它看到的都是虚拟地址空间， 在32bit CPU上，它的地址空间是4G；</p>
<p>对于CPU而言，它访问的是具体的物理内存，即物理地址空间。</p>
<p>如果由进程的虚拟机制空间来转换为CPU的物理地址空间，就有了MMU这个单元，它的作用是将虚拟地址空间映射为物理地址空间。映射方式就是采用的分页机制，首先将实际物理内存按照4K大小为单位来将物理内存划分为一个个物理页框，这些物理页框通过page结构体来索引， 所有的物理page都是通过一个字节数组mem_map[]来进行检索，该数组的每一个元素代表一个物理page的状态。 mem_map[]对于numa和uma，sparse memory和flat memroy这些内存管理模型上具有不同的实现。 numa是基于节点的， sparse memory是基于section的。</p>
<p>在地址变换的过程中，首先由虚拟地址的前12bit得出起PGD，结合PGD的内容和CP15寄存器计算出其PMD的基址，虚拟地址的中间8bit为其偏移，然后通过PMD里面的内容计算出PTE的基址（即物理页框的地址），虚拟地址的最后12bit是PTE的偏移，这个地址就是该虚拟地址对应的物理内存的地址。</p>
<p><strong>3. 虚拟地址空间的划分</strong></p>
<p>进程的地址空间分为内核空间和用户空间两部分。 这两部分的比例可以通过menuconfig来进行配置， 一般都是配置为1：3,即用户空间占3G， 内核空间占1G。</p>
<p>用户空间是0～3G这部分地址， 内核空间是3G～4G这部分地址。 对于用户空间而言，每个进程都有自己的页表，而内核空间则只有一份页表（中断／内核线程这些只运行在内核空间的都是没有mm_struct的）。</p>
<p>内核空间是如下划分：</p>
<p>内核空间起始于PAGE_OFFSET（即3G）， 在开始的区域是nomal memory区域，这部分区域包括内核镜像／mem_map[]数组，normal memory区域是线性映射， 它映射到物理内存的起始部分。</p>
<p>然后从high–memory这个地方开始就是所谓的高端内存， 在高端内存和normal memory之间有一个4K的保护页，它的主要作用是起到保护作用。</p>
<p><strong>4. 高端内存</strong></p>
<p>高端内存主要是为了解决线性地址不够用的问题。 高端内存有三种映射方式： 固定映射／永久映射／临时映射以及非连续物理地址映射。</p>
<p>非连续物理地址映射即vmalloc区域，它从VMALLOC_START到VMALLOC_END 这个区域，它的意思是线性地址是连续的，但是映射的物理地址未必是连续的，这部分空间是通过vmalloc（）来申请。</p>
<p>再往后是永久映射，就是所谓的pmap区域，它通过pmap（）来申请， 通过punmap（）来释放。</p>
<p>再往后是固定映射区， 它从fixed_addr_start开始，到fixed_addr_top结束，固定映射一般都是在编译阶段就确定好一些外设的映射关系，在系统启动阶段建立好它的映射关系后便不会在改变， 它的目的主要是解决一些外设在boot阶段就需要建立好映射的情况，一般都是用一个数学公式来表示它的映射。 都有哪些设备会用到固定映射？？ 例如中断控制器就是采用固定映射，中断入口地址就是在编译阶段确定的，对于arm而言是0xffff0000。</p>
<p>在固定映射区的后面有一个4K的保护页面    。</p>
<p>不同的映射方式之间都会有一个空隙来起到保护作用。</p>
<p><strong>5. 写时拷贝机制</strong></p>
<p>写时拷贝机制的目的， 一是为了节约物理内存，二是为了加快创建进程的速度。</p>
<p>在使用fork（）生成新的进程时，新进程与原进程会共享同一内存区，这部分内存区是只读的， 当其中一个进程进行写操作时，系统才会为其另外分配内存页面。 这就是copy－on－write的概念。</p>
<p>当进程A使用系统调用fork来创建一个子进程B时， 由于子进程B实际上是父进程A的一个拷贝，因此会拥有与父进程相同的物理页面。也即为了达到节约物理内存和快速创建的目的，fork（）函数会让子进程B以只读方式共享父进程A的物理页面，同时父进程A对这些物理页面的访问权限也设置为只读（这是通过copy_page_tables()来实现的）。 这样以来，当父进程A或者子进程B，其中任何一方对这些共享页面执行写操作的时候， 都会产生page－fault，然后cpu会执行异常处理函数do_wp_page()来试图解决这个异常。</p>
<p>首先，会对这个物理页面取消共享， 然后为写进程复制一新的物理页面， 是父进程和子进程各自拥有一块内容相同的物理页面，这时才真正的进行了复制操作（当然是只复制这一个物理页面。），然后，将要执行写操作的这个物理页面置为可写的。 最后，从异常处理函数返回，cpu会重新执行刚才导致异常的写入操作指令，使进程能够执行下去。</p>
<p><strong>6. 伙伴分配算法</strong></p>
<p>这个没有深入研究过， 也没有在实际工作中遇到这方面的问题， 所以只知道一个大概。</p>
<p>&nbsp;</p>]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[三言两语聊kernel：线程栈]]></title>
    <link href="http://laoar.github.io/blogs/250"/>
    <updated>2013-05-18T00:00:00+08:00</updated>
    <id>http://laoar.github.io/blogs/250</id>
    <content type="html"><![CDATA[<p>Oops! MacOS不支持proc文件系统，这或许是BSD系统最值得吐槽的地方吧。无奈只好到linux机器上写了这篇文章。</p>
<p>下面是一个比较简单的多线程程序。程序如下，<br />
<img src="http://laoar.github.io/images/250.jpg"><br />
上图是我的测试程序，我创建了3个线程。程序运行以后，我们可以通过/ proc/<span class="caps">PID</span>/task来看该程序有多少线程在运行：<br />
<img src="http://laoar.github.io/images/250-2.jpg"><br />
然后我们来看一下进程的地址空间。 /proc/<span class="caps">PID</span>/maps就是进程的地址空间。如下所示：<br />
<img src="http://laoar.github.io/images/250-3.jpg"><br />
可以看出，进程的地址空间从低到高依次是：进程代码段(标志含有x)、只读数据段、可读写数据段、堆、栈（包括动态库的栈空间）。<br />
<div><br />
<div>    线程83438的栈：0xb7570000 &#8211; 0xb6d70000的值恰好是8M，线程栈默认大小是8M。0xb6d70000 &#8211; 0xb6d6f000的值是1K，这1K是保护页。</div><br />
<div>    为什么这三个线程的栈都是8M？可以从ulimit命令来得出，这是进程的资源限制：</div><br />
<img src="http://laoar.github.io/images/250-4.jpg"><br />
<div>    使用ulimit <del>a命令可以看出，进程资源限制中栈大小的限制是8194K，即8M。</div><br />
<div>     那么，这个8M大小是不是可以更改的？以及后面会什么会有一个4K大小的保护页？这可以从glibc代码里面来获取答案：</div><br />
<div><br />
<div></div><br />
<div>       <strong><span style="color: #808000;">  1. </span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">__pthread_create_2_1</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">        /<strong><a><span style="color: #808000;">这里分配线程栈</span></a></strong>/</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">        ALLOCATE_STACK (iattr, &amp;pd); </span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">2. </span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">allocate_stack就是具体的分配线程栈的函数：</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">allocate_stack</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">     <a><span style="color: #808000;">/<strong>如果没有设置线程栈大小，就使用默认值</span></a></strong>/</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">     size = attr</del>&gt;stacksize ?: __default_stacksize;     </span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">     &#8230;</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">      /* Try to get a stack from the cache.  <strong>/</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">      pd = get_cached_stack (&amp;size, &amp;mem);</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">     /</strong>如果没有从cache申请到，就要mmap申请一块内存*/</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">      if (pd == <span class="caps">NULL</span>){</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">           /<strong>MAP_PRIVATE | MAP_ANONYMOUS：私有匿名映射</strong>/</span></strong></div><br />
<div style="padding-left: 30px;"><strong><span style="color: #808000;">          mmap (<span class="caps">NULL</span>, size, prot,  </span></strong></div><br />
<p style="padding-left: 120px;"><strong><span style="color: #808000;">MAP_PRIVATE | MAP_ANONYMOUS | MAP_STACK, -1, 0);   </span></strong></p></p>
<div style="padding-left: 30px;"><strong><span style="color: #808000;">      }</span></strong></div>
<div style="padding-left: 30px;"><strong><span style="color: #808000;">     /*</span></strong></div>
<div style="padding-left: 30px;"><strong><span style="color: #808000;">        接着设置一个保护区，该区域的页表属性是PROT_NONE,</span></strong></div>
<div style="padding-left: 30px;"><strong><span style="color: #808000;">        即Page can not be accessed</span></strong></div>
<div style="padding-left: 30px;"><strong><span style="color: #808000;">     */</span></strong></div>
<div style="padding-left: 30px;"><strong><span style="color: #808000;">      mprotect (guard, guardsize, PROT_NONE) </span></strong></div>
<div style="padding-left: 30px;"><strong><span style="color: #808000;"> </span></strong></div>
<div style="padding-left: 30px;"><strong><span style="color: #808000;">       对于设置为PROT_NONE的页，是不能访问的，那么访问到这个保护区时就出现错误，linux是靠这种机制来实现栈溢出保护的。</span></strong></div>
<div style="padding-left: 30px;"></div>
<div><span style="color: #000000;">    下面我们来调整线程栈：</span></div>
<div><span style="color: #000000;">   <strong> 1. 设置pthread_attr属性</strong></span></div>
</div>
<p><img src="http://laoar.github.io/images/250-5.jpg"><br />
<div></div><br />
<img src="http://laoar.github.io/images/250-6.jpg"><br />
<div><br />
<div>可以看到此时的线程栈大小是： 0xb758f000 &#8211; 0xb756f000 = 128K.</div></p>
</div>
<div>    <strong>2.通过ulimit来统一设置当前shell下将要执行的程序的线程栈 </strong></div>
<div>        ulimit -s  128</div>
<div>
<div>    要注意的是， ulimit -s是针对shell的设置， 即只对当前shell fork的进程有效。如果在另外一个shell上起进程，则是没有效果的。参见 man手册：“Control the resources available to a process started by the shell, on systems that allow such control.”</div>
</div>
<div></div>
</div>]]></content>
  </entry>
  
</feed>
